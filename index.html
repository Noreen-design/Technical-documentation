<!DOCTYPE html>
<html class="no-js" lang="en">
  <title>IBM GDPS</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">



    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">

<!-- linking 3 css stylesheets, styles is the one I've made changes to. The others are boilerplate and normalize css styles -->

    <link rel="stylesheet" href="normalize.css">
    <link rel="stylesheet" href="main-css.css">
    <link rel="stylesheet" href="styles.css">

  

  <style>
      @media only screen and (min-width: 600px) {
    .header-center {
      font-weight: 400;
    }
}
  </style>

  
  <body class="light-grey">

<!-- to launch the menu from off to left -->

    <button class="off-canvas-launcher">Menu</button>

<!-- Fixed-size-centered-content - defines a container for fixed size centered content, 
and is wrapped around the whole page content, except for the footer in this example -->

    <div class="fixed-size-centered-content">

<!-- Header -->
<!-- Header - has a class of header-footer-text-container-padding -->
<!-- Header has a class of header-center -->
<!-- Header has a class of padding-footer-header for Header and footer padding -->

    <header class="header-footer-text-container-padding header-center padding-footer-header"> 
      <h1 id="IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities"><b>IBM GDPS</b><br> An Introduction to<br> Concepts and Capabilities
      </h1>

<!-- Some styling for Redbooks, will probably replace with a graphic -->

        <p><span class="redbooks-tag1">Red</span><span class="redbooks-tag2">books</span>
        </p>
    </header>

<!-- Nav --> 
    <div class="nav-container">
      <nav id="navbar" class="navigation-menu">
        <header class="IBM-GDPS-toc-links"><b>IBM GDPS</b>
          <ul>
            <li><a href="#Introduction_to_business_resilience_and_the_role_of_GDPS" class="nav-link">Introduction to business
              resilience and the role of GDPS</a></li>
              <br>
              
            <li><a href="#Infrastructure_planning_for_availability_and_GDPS" class="nav-link">Infrastructure planning for
              availability and GDPS</a></li>
              <br>
              
            <li><a href="#GDPS_Metro" class="nav-link">GDPS Metro</a></li>
              <br>
              
            <li><a href="#GDPS_Metro_HyperSwap_Manager" class="nav-link">GDPS Metro HyperSwap
              Manager</a></li>
              <br>
              
            <li><a href="#GDPS_Global_-_XRC" class="nav-link">GDPS Global - XRC</a></li>
              <br>
              
            <li><a href="#GDPS_Global_-_GM" class="nav-link">GDPS Global - GM</a></li>
              <br>
              
            <li><a href="#GDPS_Continuous_Availability_solution" class="nav-link">GDPS Continuous Availability solution</a></li>
              <br>
              
            <li><a href="#GDPS_Virtual_Appliance" class="nav-link">GDPS Virtual Appliance</a></li>
              <br>
              
            <li><a href="#Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery" class="nav-link">
              Combining local and metro continuous availability with out of region disaster recovery</a></li>
              <br>
              
            <li><a href="#GDPS_Logical_Corruption_Protection_and_Testcopy_Manager" class="nav-link">
              GDPS Logical Corruption Protection and Testcopy Manager</a></li>
              <br>
              
            <li><a href="#Sample_continuous_availability_and_disaster_recovery_scenarios" class="nav-link">
              Sample continuous availability and disaster recovery scenarios</a></li>
              <br>
              
              
          </ul>
        </header>
      </nav>
    </div>
    

<!-- Grid -->


<!-- Chapters of the book, all in the main area -->
    <main id="main-doc" class="main-content">
<!-- 11 chapters with section tags, with classes of main-section-->
<!-- Class of chapter-box-shadow to control the box shadow -->
<!-- Class of chapter-box-margin to control the margins around the 'chapter' areas -->
<!-- Class of color-text-bground to control the color of text and background color of the 'chapter' areas -->

<!-- chapter 1 content -->


      <div class="chapter-box-shadow chapter-box-margin color-text-bground">
      <section class="main-section" id="Introduction_to_business_resilience_and_the_role_of_GDPS">
        <header class="main_chapter_heading">
          <h2><b>Introduction to business resilience and the role of GDPS</b></h2>
        </header>
          <code></code>
        
        <div class="header-footer-text-container-padding">
          <p>In this chapter, we discuss the objective of this book and briefly introduce the contents and
            layout. We discuss the topic of business IT resilience from a technical perspective (we refer to
            it as <i>IT resilience</i>).
          </p>
          <p>
            The chapter includes a general description that is not specific to mainframe platforms,
            although the topics are covered from an enterprise systems and mainframe perspective.
            Finally, we introduce the members of the IBM Geographically Dispersed Parallel Sysplex
            (GDPS) family of offerings and provide a brief description of the aspects of an IT resilience
            solution that each offering addresses.
          </p>
          <p>This chapter includes the following topics: </p>
            <ul>
              <li class="arrows">
                <a href="#Objective" class="arrow-link">Objective</a></li>
              <li class="arrows">
                <a href="#Layout_of_this_book" class="arrow-link">Layout of this book</a></li>
              <li class="arrows">
                <a href="#IT_resilience" class="arrow-link">IT resilience</a></li>
              <li class="arrows">
                <a href="#Disaster_recovery" class="arrow-link">Disaster recovery</a></li>
              <li class="arrows">
                <a href="#The_next_level" class="arrow-link">The next level</a></li>
              <li class="arrows">
                <a href="#Other_considerations" class="arrow-link">Other considerations</a></li>
              <li class="arrows">
                <a href="#Characteristics_of_an_IT_resilience_solution" class="arrow-link">Characteristics of an IT resilience solution</a></li>
              <li class="arrows">
                <a href="#GDPS_offerings" class="arrow-link">GDPS offerings</a></li>
              <li class="arrows">
                <a href="#Automation_and_disk_replication_compatibility" class="arrow-link">Automation and disk replication compatibility</a></li>
              <li class="arrows">
                <a href="#Summary" class="arrow-link">Summary</a></li>
            </ul>
        </div>

      <div class="header-footer-text-container-padding">
        <h3 id="Objective"><b>Objective</b></h3>
          <p>Business IT resilience is a high profile topic across many industries and businesses. Apart
            from the business drivers requiring near-continuous application availability, government
            regulations in various industries now take the decision about whether to have an IT resilience
            capability out of your hands.
          </p> 
          <p>This book was developed to provide an introduction to the topic of business resilience from an
            IT perspective, and to share how GDPS can help you address your IT resilience
            requirements.
          </p>
        <h3 id="Layout_of_this_book"><b>Layout of this book</b></h3>
          <p>This chapter starts by presenting an overview of IT resilience and disaster recovery. These
            practices have existed for many years. However, recently they have become more complex
            because of a steady increase in the complexity of applications, the increasingly advanced
            capabilities of available technology, competitive business environments, and government
            regulations.
          </p>
          <p>In Chapter 2, 
            <a href="#Infrastructure_planning_for_availability_and_GDPS" class="links-within-text">Infrastructure planning for availability and GDPS</a> 
            on page 13, we briefly
            describe the available technologies typically used in a GDPS solution to achieve IT resilience
            goals. To understand the positioning and capabilities of the various offerings (which
            encompass hardware, software, and services), it is also useful to have at least a basic
            understanding of the underlying technology.
          </p>
          <p>Following these two introductory chapters and starting with Chapter 3, 
            <a href="#GDPS_Metro" class="links-within-text">GDPS Metro</a> 
            on page 53, we describe the capabilities and prerequisites of each offering in the GDPS family of
            offerings. Because each offering addresses fundamentally different requirements, each
            member of the GDPS family of offerings is described in a chapter of its own.
          </p>
          <p>Finally, we include a section with examples illustrating how the various GDPS offerings can
            satisfy your requirements for IT resilience and disaster recovery.
          </p>
        <h3 id="IT_resilience"><b>IT resilience</b></h3>
          <p>IBM defines <i>IT resilience</i> as the ability to rapidly adapt and respond to any internal or external
            disruption, demand, or threat, and continue business operations without significant impact.
          </p>
          <p>IT resilience is related to, but broader in scope, than <i>disaster recovery</i>. Disaster recovery
            concentrates solely on recovering from an <i>unplanned</i> event.
          </p>
          <p>When you investigate IT resilience options, these two terms must be at the forefront of your
            thinking:
          </p>
          <ul>
            <li class="arrows">Recovery time objective (RTO)</li>
            <p class="padding-left-para">This term refers to <em>how long</em> your business can afford to wait for IT services to be resumed
              following a disaster.
            </p>

            <p class="padding-left-para">If this number is not clearly stated now, think back to the last time that you had a significant
              service outage. How long was that outage, and how much difficulty did your company
              suffer as a result? This can help you get a sense of whether to measure your RTO in days,
              hours, or minutes.
            </p> 
            <li class="arrows">Recovery point objective (RPO)</li>
            <p class="padding-left-para">This term refers to <em>how much data</em> your company is willing to re-create following a disaster.
              In other words, what is the acceptable time difference between the data in your production
              system and the data at the recovery site?
              As an example, if your disaster recovery solution depends on daily full volume tape
              dumps, your RPO is 24 - 48 hours depending on when the tapes are taken offsite. If your
              business requires an RPO of less than 24 hours, you will almost certainly be forced to do
              some form of offsite real-time data replication instead of relying on these tapes alone.</p>
          </ul>
          <p>
            The terms <i>RTO</i> and <i>RPO</i> are used repeatedly in this book because they are core concepts in
            the methodology that you can use to meet your IT resilience needs.
          </p>
        <h4 id="Disaster_recovery"><b>Disaster recovery</b></h4>
          <p>As mentioned, the practice of preparing for disaster recovery (DR) is something that has been
            a focus of IT planning for many years. In turn, there is a wide range of offerings and
            approaches available to accomplish DR. Several options rely on offsite or even outsourced
            locations that are contracted to provide data protection or even servers if there is a true IT
            disaster. Other options rely on in-house IT infrastructures and technologies that can be
            managed by your own teams.
          </p>
          <p>There is no one correct answer for which approach is better for every business. However, the
            first step in deciding what makes the most sense for <em>you</em> is to have a good view of your IT
            resiliency objectives, specifically your RPO and RTO.
          </p>
          <p>Although 
            <a href="#Table_1-1" class="links-within-text">Table 1-1</a> 
            does not cover every possible DR offering and approach, it does provide a
            view of what RPO and RTO might typically be achieved with some common options.
          </p>

<!-- Describing the table -->
          <p id="Table_1-1">
            <i>Table 1-1 Typical achievable RPO and RTO for some common DR options</i>
          </p>

<!-- Table with GRID FOR LAYOUT -->

<!-- GRIDS - one for larger screens and other for smaller - toggle display 'none' in styles.css, for when the screen size is large or small -->

<!-- GRID - for larger screens  -->
          
            <div class="disaster-recovery-grid">
              <div class="grid-item-1"><b>Description</b></div>
              <div class="grid-item-2"><b>Typically achievable recovery
                point objective (RPO)</b></div>
              <div class="grid-item-3"><b>Typically achievable recovery
                time objective (RTO)</b></div>
              <div class="grid-item-4">No disaster recovery
                plan</div>
              <div class="grid-item-5">Not applicable: all data is lost</div>
              <div class="grid-item-6">Not applicable</div>
              <div class="grid-item-7">Tape vaulting</div>
              <div class="grid-item-8">Measured in days since last stored
                backup</div>
              <div class="grid-item-9">Days</div>
              <div class="grid-item-10">Electronic vaulting</div>
              <div class="grid-item-11">Hours</div>
              <div class="grid-item-12">Hours (hot remote location) to days</div>
              <div class="grid-item-13">Active replication to
                remote site (without
                recovery automation)</div>
              <div class="grid-item-14">Seconds to minutes</div>
              <div class="grid-item-15">Hours to days (dependent on
                availability of recovery hardware)</div>
              <div class="grid-item-16">Active storage
                replication to remote
                “in-house” site</div>
              <div class="grid-item-17">Zero to minutes (dependent on
                replication technology and
                automation policy)</div>
              <div class="grid-item-18">One or more hours (dependent on
                automation)</div>
              <div class="grid-item-19">Active software
                replication to remote
                “active” site</div>
              <div class="grid-item-20">Seconds to minutes</div>
              <div class="grid-item-21">Seconds to minutes (dependent on
                automation)</div>
            </div>

<!-- GRID - for smaller screens  -->

            <div class="disaster-recovery-grid2">
              <div class="grid-item-1"><b>Description</b></div>
              <div class="grid-item-2">No disaster recovery
                plan</div>
              <div class="grid-item-3">Tape vaulting</div>
              <div class="grid-item-4">Electronic vaulting</div>
              <div class="grid-item-5">Active replication to
                remote site (without
                recovery automation)</div>
              <div class="grid-item-6">Active storage
                replication to remote
                “in-house” site</div>
              <div class="grid-item-7">Active software
                replication to remote
                “active” site</div>
              <div class="grid-item-8">Typically achievable recovery
                point objective (RPO)</div>
              <div class="grid-item-9">Not applicable: all data is lost</div>
              <div class="grid-item-10">Measured in days since last stored
                backup</div>
              <div class="grid-item-11">Hours</div>
              <div class="grid-item-12">Seconds to minutes</div>
              <div class="grid-item-13">Zero to minutes (dependent on
                replication technology and
                automation policy)</div>
              <div class="grid-item-14">Seconds to minutes</div>
              <div class="grid-item-15">Typically achievable recovery
                time objective (RTO)</div>
              <div class="grid-item-16">Not applicable
                </div>
              <div class="grid-item-17">Days</div>
              <div class="grid-item-18">Hours (hot remote location) to days</div>
              <div class="grid-item-19">Hours to days (dependent on
                availability of recovery hardware)</div>
              <div class="grid-item-20">One or more hours (dependent on
                automation)</div>
              <div class="grid-item-21">Seconds to minutes (dependent on
                automation)</div>
            </div> 
          <p>
            Generally a form of real-time software or hardware replication is required to achieve an RPO
            of minutes or less, but the only technologies that can provide an RPO of zero are
            synchronous replication technologies (see 2.3, 
            <a href="#Synchronous_versus_asynchronous_data_transfer" class="links-within-text">Synchronous versus asynchronous data
            transfer</a> on page 19) coupled with automation to ensure that no data is written to one location
            and not the other.
          </p> 
          <p>
            The recovery time is largely dependent on the availability of hardware to support the recovery
            and control over that hardware. You might have real-time software or hardware-based
            replication in place, but without server capacity at the recovery site you will have hours to
            days before you can recover this previously current data.
          </p>
          <p>Furthermore, even with all the spare capacity and current data, you might find that you are
            relying on people to perform the recovery actions. In this case, you will undoubtedly find that
            these same people are not necessarily available in the case of a true disaster or, even more
            likely, they find that processes and procedures for the recovery are not practiced or accurate.
            This is where automation comes in to mitigate the risk introduced by the human element and
            to ensure that you actually meet the RTO required of the business.
          </p>  
          <p>Also, you might decide that one DR option is not appropriate for all aspects of the business.
            Various applications might tolerate a much greater loss of data and might not have an RPO as
            low as others. At the same time, some applications might not require recovery within hours
            whereas others most certainly do.
          </p>
          <p>Although there is obvious flexibility in choosing different DR solutions for each application, the
            added complexity this can bring needs to be balanced carefully against the business benefit.
            The preferred approach, supported by GDPS, is to provide a single optimized solution for the
            enterprise. This generally leads to a simpler solution and, because less infrastructure and
            software might need to be duplicated, often a more cost-effective solution, too. Consider a
            different DR solution only for your most critical applications, where their requirements cannot
            be catered for with a single solution.
          </p>       
        <h4 id="The_next_level"><b>The next level</b></h4>
          <p>In addition to the ability to recover from a disaster, many businesses now look for a greater
            level of availability covering a wider range of events and scenarios. This larger requirement is
            called <i>IT resilience</i>. In this book, we concentrate on two aspects of IT resilience: <i>Disaster
            recovery</i>, as discussed previously, and <i>continuous availability</i> (CA), which encompasses
            recovering from disasters and keeping your applications up and running throughout the far
            more common planned and unplanned outages that do not constitute an actual disaster.
          </p>
          <p>For some organizations, a proven disaster recovery capability that meets their RTO and RPO
            can be sufficient. Other organizations might need to go a step further and provide
            near-continuous application availability.
          </p>
          <p>There are several market factors that make IT resilience imperative:
          </p>
          <ul>
            <li class="arrows">High and constantly increasing client and market requirements for continuous availability
              of IT processes 
            </li>
            <li class="arrows">Financial loss because of lost revenue, punitive penalties or fines, or legal actions that are
              a direct result of disruption to critical business services and functions
            </li>
            <li class="arrows">An increasing number of security-related incidents, causing severe business impacts
            </li>
            <li class="arrows">Increasing regulatory requirements
            </li>
            <li class="arrows">Major potential business impact in areas such as market reputation and brand image from
              security or outage incidents
            </li>
          </ul>
          <p>For a business today, few events affect a company as much as having an IT outage, even for
            a matter of minutes, and then finding a report of the incident splashed across the newspapers
            and the evening news. Today, your clients, employees, and suppliers expect to be able to do
            business with you around the clock and from around the globe. 
          </p>
          <p>To help keep business operations running 24x7, you need a comprehensive business
            continuity plan that goes beyond disaster recovery. Maintaining high availability and
            continuous operations in normal day-to-day operations are also fundamental for success.
            Businesses need resiliency to help ensure two essentials:       
          </p>
          <ul>
            <li class="arrows">Key business applications and data are protected and available
            </li>
            <li class="arrows">If a disaster occurs, business operations continue with a minimal impact
            </li>
          </ul>

<!-- A couple of links now in Regulations - previously - they were in the pdf footer -->

        <h5><b>Regulations</b></h5>
          <p>In some countries, government regulations specify how organizations must handle data and
            business processes. An example is the Health Insurance Portability and Accountability Act
            (HIPAA) in the United States. This law defines how an entire industry, the US healthcare
            industry, must handle and account for patient-related data.
          </p>
          <p>Other well-known examples include the US government-released,
            <a href="https://www.sec.gov/news/studies/34-47638.htm"><i>Interagency Paper on
              Sound Practices to Strengthen the Resilience of the US Financial System</i></a>
            which loosely
            drove changes in the interpretation of IT resilience within the US financial industry, and the
            Basel II rules for the European banking sector, which stipulate that banks must have a
            resilient back-office infrastructure.
          </p>
          <p>This is also an area that accelerates as financial systems around the world become more
            interconnected. Although a set of recommendations published in Singapore (such as the
            <a href="https://www.singaporestandardseshop.sg/"><i>S 540-2008 Standard on Business Continuity Management)</i></a>
            might be directly addressing
            only businesses in a relatively small area, it is common for companies to do business in many
            countries around the world, where these might be requirements for ongoing business
            operations of any kind.
          </p>
        <h5><b>Business requirements</b></h5>
          <p>An important concept to understand is that the cost and complexity of a solution can increase
            as you get closer to true continuous availability, and that the value of a potential loss must be
            borne in mind when deciding which solution you <em>need</em>, and which one you can <em>afford</em>. You do
            not want to spend more money on a continuous availability solution than the financial loss you
            can incur as a result of an outage.
          </p>
          <p>A solution must be identified that balances the costs of the solution with the financial impact
            of an outage. Several studies have been done to identify the cost of an outage; however, most
            of them are several years old and do not accurately reflect the degree of dependence most
            modern businesses have on their IT systems.
          </p>
          <p>Therefore, your company must calculate the impact in your specific case. If you have not
            already conducted such an exercise, you might be surprised at how difficult it is to arrive at an
            accurate number. For example, if you are a retailer and you suffer an outage in the middle of
            the night after all the batch work has completed, the financial impact is far less than if you had
            an outage of equal duration in the middle of your busiest shopping day. Nevertheless, to
            understand the value of the solution, you must go through this exercise, using assumptions
            that are fair and reasonable.
          </p>
        <h4 id="Other_considerations"><b>Other considerations</b></h4>
          <p>In addition to the increasingly stringent availability requirements for traditional mainframe
            applications, there are other considerations, including those described in this section.
          </p> 
        <h5><b>Increasing application complexity</b></h5>
          <p>The mixture of disparate platforms, operating systems, and communication protocols found
            within most organizations intensifies the already complex task of preserving and recovering
            business operations. Reliable processes are required for recovering the mainframe data and
            also, perhaps, data accessed by multiple types of UNIX, Microsoft Windows, or even a
            proliferation of virtualized distributed servers.
          </p>
          <p>It is becoming increasingly common to have business transactions that span and update data
            on multiple platforms and operating systems. If a disaster occurs, your processes must be
            designed to recover this data in a consistent manner.
          </p>
          <p>Just as you would not consider recovering half an application’s IBM DB2® data to 8:00 a.m.
            and the other half to 5:00 p.m., the data touched by these distributed applications must be
            managed to ensure that <em>all</em> of this data is recovered with consistency to a single point in time.
            The exponential growth in the amount of data generated by today’s business processes and
            IT servers compounds this challenge.
          </p>
        <h5><b>Increasing infrastructure complexity</b></h5>
          <p>Have you looked in your computer room recently? If you have, you probably found that your
            mainframe systems are only a small part of the equipment in that room. How confident are
            you that all those other platforms can be recovered? And if they can be recovered, will it be to
            the same point in time as your mainframe systems? And how long will that recovery take?
          </p>
          <p><a href="#Figure_1_1" class="links-within-text">Figure 1-1</a>
            shows a typical IT infrastructure. If you have a disaster and recover the mainframe
            systems, will you be able to recover your service without all the other components that sit
            between the user and those systems? It is important to remember why you want your
            applications to be available, so that users can access them.
          </p>
          <p>Therefore, part of your IT resilience solution must include more than addressing the
            non-mainframe parts of your infrastructure. It must also ensure that recovery is integrated
            with the mainframe plan.
          </p>

<!-- Screenshot taken from the pdf - so will NEED the real graphic at some point -->

        <div class="image_container">
          <figure>
            <img src="typical_IT_infrastructure.jpg" class="IT-infra"
                 alt="IT infrastructure graphic">
            <figcaption id="Figure_1_1"><em>Figure 1-1 Typical IT infrastructure</em></figcaption>
        </figure>

        </div>
        <h5><b>Outage types</b></h5>
          <p>In the early days of computer data processing, planned outages were relatively easy to
            schedule. Most of the users of your systems were within your company, so the impact to
            system availability was able to be communicated to all users in advance of the outage.
            Examples of planned outages are software or hardware upgrades that require the system to
            be brought down. These outages can take minutes or even hours.
          </p>
          <p>Most outages are planned, and even among unplanned outages, most are not disasters.
            However, in the current business world of 24x7 Internet presence and web-based services
            shared across and also between enterprises, even planned outages can be a serious
            disruption to your business.
          </p>
          <p>Unplanned outages are unexpected events. Examples of unplanned outages are software or
            hardware failures. Although various of these outages might be quickly recovered from, others
            might be considered a disaster.
          </p>
          <p>You will undoubtedly have both planned and unplanned outages while running your
            organization, and your business resiliency processes must cater to both types. You will likely
            find, however, that coordinated efforts to reduce the numbers of and impacts of unplanned
            outages often are complementary to doing the same for planned outages.
          </p>
          <p>Later in this book we discuss the technologies available to you to make your organization
            more resilient to outages, and perhaps avoid them altogether. 
          </p>
        <h4 id="Characteristics_of_an_IT_resilience_solution"><b>Characteristics of an IT resilience solution</b></h4>
          <p>As the previous sections demonstrate, IT resilience encompasses much more than the ability
            to get your applications up and running after a disaster with “some” amount of data loss, and
            after “some” amount of time.
          </p>
          <p>When investigating an IT resilience solution, keep in mind the following points:
          </p>
           <ul>
            <li class="arrows">Support for planned system outages</li>
              <p class="padding-left-para">Does the proposed solution provide the ability to stop a system in an orderly manner?
              Does it provide the ability to move a system from the production site to the backup site in a
              planned manner? Does it support server clustering, data sharing, and workload balancing,
              so the planned outage can be masked from users?
          </p>
            <li class="arrows">Support for planned site outages</li>
            <p class="padding-left-para">Does the proposed solution provide the ability to move the entire production environment
              (systems, software subsystems, applications, and data) from the production site to the
              recovery site? Does it provide the ability to move production systems back and forth
              between production and recovery sites with minimal or no manual intervention?
            </p>
            <li class="arrows">Support for data that spans more than one platform</li>
            <p class="padding-left-para">Does the solution support data from more systems than just z/OS? Does it provide data
              consistency across all supported platforms, or only within the data from each platform?
            </p>
            <li class="arrows">Support for managing the data replication environment</li>
            <p class="padding-left-para">Does the solution provide an easy-to-use interface for monitoring and managing the data
              replication environment? Will it automatically react to connectivity or other failures in the
              overall configuration?
            </p>
            <li class="arrows">Support for data consistency</li>
            <p class="padding-left-para">Does the solution provide consistency across all replicated data? Does it provide support
              for protecting the consistency of the second copy if it is necessary to resynchronize the
              primary and secondary copy?
            </p>
            <li class="arrows">Support for continuous application availability</li>
            <p class="padding-left-para">Does the solution support continuous application availability? From the failure of any
              component? From the failure of a complete site? 
            </p>
            <li class="arrows">Support for hardware failures</li>
            <p class="padding-left-para">Does the solution support recovery from a hardware failure? Is the recovery disruptive
              (reboot or IPL again) or transparent (HyperSwap, for example)?
            </p>
            <li class="arrows">Support for monitoring the production environment</li>
            <p class="padding-left-para">Does the solution provide monitoring of the production environment? Is the operator
              notified in a failure? Can recovery be automated?
            </p>
            <li class="arrows">Dynamic provisioning of resources</li>
            <p class="padding-left-para">Does the solution have the ability to dynamically allocate resources and manage
              workloads? Will critical workloads continue to meet their service objectives, based on
              business priorities, if there is a failure?
            </p>
            <li class="arrows">Support for recovery across database managers</li>
            <p class="padding-left-para">Does the solution provide recovery with consistency independent of the database
              manager? Does it provide data consistency across multiple database managers?
            </p>
            <li class="arrows">End-to-end recovery support</li>
            <p class="padding-left-para">Does the solution cover all aspects of recovery, from protecting the data through backups
              or remote copy, through to automatically bringing up the systems following a disaster?
            </p>
            <li class="arrows">Cloned applications</li>
            <p class="padding-left-para">Do your critical applications support data sharing and workload balancing, enabling them
              to run concurrently in more than one site? If so, does the solution support and use this
              capability?
            </p>
            <li class="arrows">Support for recovery from regional disasters</li>
              <p class="padding-left-para">What distances are supported by the solution? What is the impact on response times?
              Does the distance required for protection from regional disasters permit a continuous
              application availability capability?
            </p>
          </ul>
          <p>
            You then need to compare your company’s requirements in each of these categories against
            your existing or proposed solution for providing IT resilience.
          </p>
        <h4 id="GDPS_offerings"><b>GDPS offerings</b></h4>
          <p>GDPS is a collection of several offerings, each addressing a different set of IT resiliency goals
            that can be tailored to meet the RPO and RTO for your business. Each offering uses a
            combination of server and storage hardware or software-based replication and automation
            and clustering software technologies, many of which are described in more detail in
            Chapter 2, 
            <a href="#Infrastructure_planning_for_availability_and_GDPS" class="links-within-text">
            Infrastructure planning for availability and GDPS</a> 
            on page 13.
          </p>
          <p>In addition to the infrastructure that makes up a given GDPS solution, IBM also includes
            services, particularly for the first installation of GDPS and optionally for subsequent
            installations to ensure that the solution meets and fulfills your business objectives. 
          </p>
          <p>The following list briefly describes each offering, with a view of which IT resiliency objectives it
            is intended to address. Extra details are included in separate chapters of this book:
          </p>
          <ul>
            <li class="arrows">GDPS Metro</li>
            <p class="padding-left-para">A near-CA and DR solution across two sites separated by metropolitan distances. The
            solution is based on the IBM Metro Mirror synchronous disk mirroring technology.</p>
           
            <li class="arrows">GDPS Metro HyperSwap Manager</li>
            <p class="padding-left-para">A near-CA solution for a single site or entry-level DR solution that is across two sites
            separated by metropolitan distances. The solution is based on the same mirroring
            technology as GDPS Metro, but does not include much of the system automation
            capability that makes GDPS Metro a more complete DR solution.
            </p>
            <li class="arrows">IBM GDPS Virtual Appliance </li>
            <p class="padding-left-para">A near-CA and DR solution across two sites that are separated by metropolitan distances.
            The solution is based on the IBM Metro Mirror synchronous disk mirroring technology. The
            solution provides Near-CA <em>and</em> DR protection for IBM z/VM and Linux on IBM Z in
            environments that do not have IBM z/OS operating systems.
            </p>
            <li class="arrows">GDPS Global - XRC (also known as GDPS XRC)</li>
            <p class="padding-left-para">A DR solution across two regions that are separated by virtually unlimited distance. The
            solution is based on the IBM Extended Remote Copy (XRC) asynchronous disk mirroring
            technology (also called IBM z/OS Global Mirror).
            </p>
            <li class="arrows">GDPS Global - GM (also known as GDPS GM)</li>
            <p class="padding-left-para">A DR solution across two regions that are separated by virtually unlimited distance. The
            solution is based on the IBM System Storage Global Mirror technology, which is a disk
            subsystem-based asynchronous form of remote copy.
            </p>
            <li class="arrows">GDPS Metro Global - GM (also known as GDPS MGM)</li>
            <p class="padding-left-para">A 3-site or a symmetrical 4-site configuration is supported:</p>
            <li class="dash">GDPS MGM 3-site</li> 
            <p class="padding-left-para">A 3-site solution that provides CA across two sites within metropolitan distances in one
            region and DR to a third site, in a second region, at virtually unlimited distances. It is
            based on a combination of the Metro Mirror and Global Mirror technologies.
            </p>
            <li class="dash">GDPS MGM 4-site</li>
            <p class="padding-left-para"></p>A symmetrical 4-site solution that is similar to the 3-site solution in that it provides CA
            within region and DR cross region. In addition, in the 4-site solution, the two regions
            are configured symmetrical so that the same levels of CA and DR protection is
            provided, no matter which region production runs in.  
            </p>
            <li class="arrows">GDPS Metro Global - XRC (also known as GDPS MzGM)</li>
            <li class="dash">GDPS MzGM 3-site</li> 
            <p class="padding-left-para">A 3-site solution that provides CA across two sites within metropolitan distances in one
              region and DR to a third site in a second region at virtually unlimited distances. It is
              based on a combination of the Metro Mirror and XRC technologies.</p>
            <li class="dash">GDPS MzGM 4-site</li>
            <p class="padding-left-para">A symmetrical 4-site solution that is similar to the 3-site solution in that it provides CA
              within region and DR cross region. In addition, in the 4-site solution, the two regions
              are configured symmetrically so that the same levels of CA and DR protection is
              provided, no matter which region that production runs in.</p> 
           
            <li class="arrows">GDPS Continuous Availability</li>
            <p class="padding-left-para">A multisite CA/DR solution at virtually unlimited distances. This solution is based on
              software-based asynchronous mirroring between two active production sysplexes running
              the same applications with the ability to process workloads in either site.</p>         
            <p>
            As mentioned briefly at the beginning of this section, each of these offerings provides the
            following benefits:
            </p>
            <li class="arrows">GDPS automation code</li>
            <p class="padding-left-para">This code has been developed and enhanced over several years to use new hardware and
            software capabilities to reflect preferred practices, based on IBM experience with GDPS
            clients since the inception of GDPS, in 1998, and to address the constantly changing
            requirements of our clients.
            </p>
            <li class="arrows">Can use underlying hardware and software capabilities</li>
            <p class="padding-left-para">IBM software and hardware products have support to surface problems that can affect the
            availability of those components, and to facilitate repair actions.
            </p>
            <li class="arrows">Services</li>
            <p class="padding-left-para">There is perhaps only one factor in common across all GDPS implementations, namely
            that each has a unique requirement or attribute that makes it different from every other
            implementation. The services aspect of each offering provides you with invaluable access
            to experienced GDPS practitioners.
            </p>
            <p class="padding-left-para">The amount of service included depends on the scope of the offering. For example, more
            function-rich offerings, such as GDPS Metro, include a larger services component than
            GDPS Metro HyperSwap Manager.
            </p>
          </ul>
         
<!-- A formatted note -->

        <div class="note-container">
          <p role="note" class="info_repeated_multiple_chapters">
            <strong>Note: </strong>Detailed information about each of the offerings is provided in the following chapters.
            It is not necessary to read all chapters if you are interested only in a specific offering. If you
            do read all of the chapters, you might notice that some information is repeated in multiple
            chapters.
          </p>
        </div>

        <h4 id="Automation_and_disk_replication_compatibility"><b>Automation and disk replication compatibility</b></h4>
          <p>The GDPS automation code relies on the runtime capabilities of IBM Z NetView and IBM
            System Automation. Although these products provide tremendous first-level automation
            capabilities in and of themselves, there are alternative solutions you might already have from
            other vendors.
          </p>
          <p>GDPS continues to deliver features and functions that take advantage of properties unique to
            the IBM Tivoli products (such as support for alert management through IBM System
            Automation for Integrated Operations Management), but Z NetView and IBM System
            Automation also work well alongside other first-level automation solutions. Therefore,
            although benefits exist to having a comprehensive solution from IBM, you do not have to
            replace your current automation investments before moving forward with a GDPS solution.
          </p>  
          <p>Most of the GDPS solutions rely on the IBM developed disk replication technologies 
            <b>(Disk replication technology is independent of the GDPS Continuous Availability solution, which uses software
            replication)</b> of Metro
            Mirror for GDPS Metro, XRC for GDPS XRC, and Global Mirror for GDPS GM. These
            architectures are implemented on IBM enterprise disk storage products. Also, the external
            interfaces for all of these disk replication technologies (Metro Mirror, XRC, GM, and
            FlashCopy) were licensed by many major enterprise storage vendors. 
          </p>

<!-- A piece of reference text that was in the footer of the pdf -->

          <p>
            3 Disk replication technology is independent of the GDPS Continuous Availability solution, which uses software
            replication.
          </p>
          <p>
            This gives clients the flexibility to select the disk subsystems that best match their
            requirements and to mix and match disk subsystems from different storage vendors within the
            context of a single GDPS solution. Although most GDPS installations do rely on IBM storage
            products, there are several production installations of GDPS around the world that rely on
            storage products from other vendors. 
          </p>
          <p>IBM has a <a href="https://www.ibm.com/it-infrastructure/z/technologies/gdps">GDPS Qualification Program</a> for other enterprise storage vendors to validate that
            their implementation of the advanced copy services architecture meets the GDPS
            requirements.
          </p>
          <p>The GDPS Qualification Program offers the following arrangement to vendors:
          </p>
          <ul>
           <li class="arrows">IBM provides the system environment.
           </li>
           <li class="arrows">Vendors install their disk in this environment.
           </li>
           <li class="arrows">Testing is conducted jointly.
           </li>
           <li class="arrows">A qualification report is produced jointly, describing details of what was tested and results.
           </li>
          </ul>
          <p>
            Recognize that this qualification program does not imply that IBM provides defect or
            troubleshooting support for a qualified vendor’s products. It does, however, indicate at least a
            point-in-time validation that the products are functionally compatible and demonstrates that
            they work in a GDPS solution.
          </p> 
          <p>Check directly with non-IBM storage vendors if you are considering using their products with
            a GDPS solution because they can share their own approaches and capability to support the
            specific GDPS offering you are interested in.
          </p>

        <h4 id="Summary"><b>Summary</b></h4>
          <p>At this point we have discussed why it is important to have an IT resilience solution, and have
            provided information about key objectives to consider when developing your own solution. We
            have also introduced the GDPS family of offerings with a brief description of which objectives
            of IT resiliency each offering is intended to address.
          </p>
          <p>In Chapter 2,
            <a href="#Infrastructure_planning_for_availability_and_GDPS" class="links-within-text">
              Infrastructure planning for availability and GDPS</a> on page 13 we introduce key
            infrastructure technologies related to IT resilience focused on the mainframe platform. After
            that, we describe how the various GDPS offerings use those technologies. And finally, we
            position the various GDPS offerings against typical business scenarios and requirements.
          </p>
          <p>We intend to update this book as new GDPS capabilities are delivered. 
          </p>

<!-- A piece of reference text that was in the footer of the pdf -->
          <p>
            <a href="https://www.ibm.com/it-infrastructure/z/technologies/gdps" target="_blank" class="links-within-text">IBM IT infrastructure</a>
          </p>

        </div>


<!-- END OF CHAPTER 1, the 5 button type links to jump to previous chapter, the next chapter and to 
  skip to the next chapter again, to jump to the top and bottom of the document -->
        <div class="prev-next">
          <div class="button-type-container"></div>
            <a href="#Introduction_to_business_resilience_and_the_role_of_GDPS" class="button-type chapter-1">Top of section</a>
        
          <div class="button-type-container"></div>
            <a href="#Infrastructure_planning_for_availability_and_GDPS" class="button-type to-chapter-2">Next »</a>
          <div class="button-type-container"></div>
            <a href="#GDPS_Metro" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

          <div class="button-type-container"></div>
            <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

          <div class="button-type-container"></div>
            <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
        </div>
    </section>
  </div> 
  <hr>
 





<!-- chapter 2  -->


<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section" id="Infrastructure_planning_for_availability_and_GDPS">
    <header class="main_chapter_heading" >
      <h2><b>Infrastructure planning for
        availability and GDPS</b></h2>
    </header>
      <code></code>
      <ul>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
      </ul> 
    
    <div class="header-footer-text-container-padding">
      <p>In this chapter, we discuss several technologies that are available to help you achieve your
        goals related to IT resilience, recovery time, and recovery point objectives. To understand
        how the IBM GDPS offerings described in this book can help you, it is important to have at
        least conceptual knowledge of the functions, capabilities, and limitations of these underlying
        technologies.
      </p>
      <p>This chapter includes the following topics: </p>
        <ul>
          <li class="arrows">
            <a href="#Parallel_Sysplex_overview" class="arrow-link">Parallel Sysplex overview</a></li>
          <li class="arrows">
            <a href="#Data_consistency" class="arrow-link">Data consistency</a></li>
          <li class="arrows">
            <a href="#Synchronous_versus_asynchronous_data_transfer" class="arrow-link">Synchronous versus asynchronous data transfer</a></li>
          <li class="arrows">
            <a href="#Data_replication_technologies" class="arrow-link">Data replication technologies</a></li>
          <li class="arrows">
            <a href="#Tape_resident_data" class="arrow-link">Tape resident data</a></li>
          <li class="arrows">
            <a href="#FlashCopy" class="arrow-link">FlashCopy</a></li>
          <li class="arrows">
            <a href="#Automation" class="arrow-link">Automation</a></li>
          <li class="arrows">
            <a href="#Flexible_server_capacity" class="arrow-link">Flexible server capacity</a></li>
          <li class="arrows">
            <a href="#Cross_-_site_connectivity_considerations" class="arrow-link">Cross-site connectivity considerations</a></li>
          <li class="arrows">
            <a href="#Testing_considerations" class="arrow-link">Testing considerations</a></li>
          <li class="arrows">
            <a href="#summary_chapter2" class="arrow-link">Summary</a></li>
        </ul>
    </div>

  <div class="header-footer-text-container-padding">
    <h3 id="Parallel_Sysplex_overview"><b>Parallel Sysplex overview</b></h3>
      <p>As discussed in Chapter 1,
        <a href="#Introduction_to_business_resilience_and_the_role_of_GDPS">Introduction to business resilience 
          and the role of GDPS
        </a>, 
        <i>IT resilience</i> covers more than just recovery from a disaster. It also encompasses
        ensuring high availability on a day-to-day basis, protecting your applications from normal
        planned, and unplanned outages. You cannot expect to be able to provide continuous or
        near-continuous application availability across a disaster if you are unable to provide that in
        normal operations. 
      </p> 
      <p>Parallel Sysplex is the primary mechanism that is used by IBM to provide the highest levels of
        application availability on the IBM Z platform. 
        <em>In this book, we use the term IBM Z to refer to the z Systems, IBM z Systems, System z, and zSeries ranges of
          processors. If something applies only to System z or zSeries processors, we point that out at the time.</em>
        The logical first step in a business resiliency
        project is to do all you can to deliver the highest levels of service from your existing
        configuration. Implementing Parallel Sysplex with data sharing and dynamic workload routing
        provides higher levels of availability now. It also provides a foundation to achieve greater
        resiliency if you implement GDPS.
      </p>
      <p>In the following sections we briefly discuss Parallel Sysplex, the benefits you can derive by
        using the technology, and the points to consider if you decide to implement GDPS Metro or
        GDPS Continuous Availability. Because GDPS XRC and GDPS GM do not have a continuous
        availability (CA) aspect, there are no Parallel Sysplex considerations specifically relating to
        GDPS XRC and GDPS GM. There are also no Parallel Sysplex considerations for the IBM
        GDPS Virtual Appliance because the GDPS Virtual Appliance protects only IBM z/VM and
        Linux on IBM Z platforms.
      </p>
    <h4 id="Maximizing_application_availability"><b>Maximizing application availability</b></h4>
      <p>There is only one way to protect applications from the loss of a single component (such as an
        IBM CICS region or a z/OS system), and that is to run multiple, failure-isolated copies. This
        infers an ability to share data at the record level, with integrity, and to <i>dynamically</i> route
        incoming work requests across the available servers. Parallel Sysplex uses hardware and
        software components to link individual systems together in a cluster. Because all systems in
        the sysplex are able to share the same resources and data, they appear as a single image to
        applications and users, while providing the ability to eliminate single points of failure. 
      </p>
      <p>Having more than one instance of an application within the sysplex can shield your users from
        both planned and unplanned outages. With Parallel Sysplex, parts of the cluster can be
        brought down for maintenance, upgrades, or any other type of outage, while the applications
        continue to be available on other members of the sysplex. 
      </p>
      <p>GDPS Continuous Availability further extends this concept with the ability to switch the
        workload between two sysplexes separated by virtually unlimited distance for both planned
        and unplanned outage situations. 
      </p>
      <p>Although it is not necessary to have a Parallel Sysplex before implementing most GDPS
        solutions, it is important to understand the role that Parallel Sysplex plays in supporting the
        continuous availability aspect of IT resilience. Technical information about implementing and
        using Parallel Sysplex is available in other IBM documentation, so it is not covered in this
        book.  
      </p>
    <h4 id="Multisite_sysplex_considerations"><b>Multisite sysplex considerations</b></h4>
      <p>The considerations for a multisite sysplex depend on whether you plan to run production
        systems in both sites at the same time or if all the production systems will be in a single site at
        any one time. Configurations where production systems can run in both sites at the same
        time are referred to as 
        <code>multisite workload configurations</code>. 
        Configurations where the
        production systems run together in one site or the other (but not split across multiple sites)
        are referred to as 
        <code>single-site workload 
          configurations</code> or sometimes as 
        <code>Active/Standby configurations</code>. Other variations on this, where production systems are predominantly running
        at one site but where partially active systems or systems enabled only for queries are running
        at the other site, are still considered multisite workloads. 
      </p>

<!-- A formatted note -->

    <div class="note-container">
      <p role="note" class="terminology">
        <strong>Terminology: </strong>This section is focused on a <i>multisite sysplex</i>, which is a single sysplex
        spread across multiple (typically two) sites, and how the workload is configured to run in
        those sites to provide near-continuous availability and metro distance DR.
     <br>
     <br>
        Do not confuse it with the GDPS Continuous Availability solution that uses some of the
        same terminology, but is related to multiple sysplexes (limited to two, currently) and how
        the workload is configured between the two sysplexes, not within any single sysplex. 
      <br>
     <br>
        In a GDPS Continuous Availability environment, it is anticipated that each of the
        participating sysplexes are in an Active/Active configuration. This configuration provides
        local and continuous availability with GDPS Metro and GDPS Continuous Availability,
        which provides a solution for unlimited distance CA/DR. For more information about the
        GDPS Continuous Availability solution, see 
        <a href="#GDPS_Continuous_Availability_solution">GDPS Continuous Availability solution.</a>
      </p>
    </div>

      <p>
      Several phrases are often used to describe variations of multisite workload. Brief definitions
      are included here for the more commonly implemented variations.
      </p>

<!-- Tablelike GRID FOR LAYOUT -->

<!-- GRIDS - one for larger screens and other for smaller - toggle display 'none' in styles.css -->

<!-- GRID - for larger screens  -->
      
    <div class="active-grid-large-screen">
      <div class="grid-item-active1"><b>Active/Active</b></div>
      <div class="grid-item-active2">This refers to a multisite workload configuration where z/OS systems
      are actively running in the same sysplex with active subsystems in
      more than one site at the same time. Typically this term also implies
      that applications take advantage of data sharing and dynamic
      workload routing in such a way that applications can freely move from
      one site to another. Finally, critical Parallel Sysplex resources are
      duplexed or replicated in such a way that if one site fails, the remaining
      site can recover workload within minutes after contending locks and
      communications timeouts clear. When combined with HyperSwap, an
      Active/Active configuration has the potential to provide
      near-continuous availability for applications even in the case of a site
      outage.</div>
      <div class="grid-item-active3"><b>Active/Warm</b></div>
      <div class="grid-item-active4">This refers to a multisite workload configuration that is similar to the
      Active/Active configuration, with production systems running at more
      than one site. The difference is that workload generally runs in one site
      at a time, with the systems in the other site simply IPLed without
      subsystems or other resources active.
      <br>
      <br>
      This configuration is intended to save IPL time when moving workload
      between sites. It can be most effective for supporting the planned
      movement of workload because in many unplanned scenarios, the
      “warm” systems might also not survive.
      </div>
      <div class="grid-item-active5"><b>Active/Query</b></div>
      <div class="grid-item-active6">This refers to a multisite workload configuration that is quite close to
      the Active/Active configuration, but where workload at the second site
      is partitioned or restricted (possibly to queries only) in such a way as
      to limit impacts because of serialization, thereby protecting shared
      resources when delays because of distance between the sites is a
      concern. Again, depending on the configuration of the coupling facility
      structures (that is, whether they are duplexed across sites or basically
      in one site at a time), this configuration might provide value only for
      planned scenarios because in many unplanned scenarios the “query”
      or “hot standby” subsystems might not survive.</div>

    </div>

<!-- GRID - for smaller screens  -->

  <div class="active-grid-small-screen">
    <div class="grid-item-active1"><b>Active/Active</b></div>
    <div class="grid-item-active2">This refers to a multisite workload configuration where z/OS systems
      are actively running in the same sysplex with active subsystems in
      more than one site at the same time. Typically this term also implies
      that applications take advantage of data sharing and dynamic
      workload routing in such a way that applications can freely move from
      one site to another. Finally, critical Parallel Sysplex resources are
      duplexed or replicated in such a way that if one site fails, the remaining
      site can recover workload within minutes after contending locks and
      communications timeouts clear. When combined with HyperSwap, an
      Active/Active configuration has the potential to provide
      near-continuous availability for applications even in the case of a site
      outage.</div>
    <div class="grid-item-active3"><b>Active/Warm</b></div>
    <div class="grid-item-active4">This refers to a multisite workload configuration that is similar to the
      Active/Active configuration, with production systems running at more
      than one site. The difference is that workload generally runs in one site
      at a time, with the systems in the other site simply IPLed without
      subsystems or other resources active.
      <br>
      <br>
      This configuration is intended to save IPL time when moving workload
      between sites. It can be most effective for supporting the planned
      movement of workload because in many unplanned scenarios, the
      “warm” systems might also not survive.</div>
    <div class="grid-item-active5"><b>Active/Query</b></div>
    <div class="grid-item-active6">This refers to a multisite workload configuration that is quite close to
      the Active/Active configuration, but where workload at the second site
      is partitioned or restricted (possibly to queries only) in such a way as
      to limit impacts because of serialization, thereby protecting shared
      resources when delays because of distance between the sites is a
      concern. Again, depending on the configuration of the coupling facility
      structures (that is, whether they are duplexed across sites or basically
      in one site at a time), this configuration might provide value only for
      planned scenarios because in many unplanned scenarios the “query”
      or “hot standby” subsystems might not survive.</div>
  </div> 


  <p>
    You can devise potentially many more configuration variations, but from a Parallel Sysplex
    and GDPS <b>(not including the GDPS Continuous Availability solution which relates to a multiple sysplex configuration that can
    be either single-site or multisite workloads)</b> perspective, all of these fall into either the single-site or the multisite workload
    category.
  </p>

  <h5><b>Single-site or multisite workload configuration</b></h5>
  <p>When first introduced, Parallel Sysplexes were typically contained within a single site.
    Extending the distance between the operating system images and the coupling facility has an
    impact on the response time of requests using that coupling facility (CF). Also, even if the
    systems sharing the data are spread across more than one site, all of the primary disk
    subsystems are normally contained in the same site, so a failure affecting the primary disks
    affects the systems in both sites. As a result, a multisite workload configuration does not, in
    itself, provide significantly greater availability than a single-site workload configuration during
    unplanned outages. To achieve the optimal benefit from a multisite workload configuration for
    planned outages, HyperSwap should be used; this enables you to move applications and their
    data from one site to the other nondisruptively.
  </p>
  <p>More specifically, be careful when planning a multisite workload configuration if the underlying
    Parallel Sysplex cannot be configured to spread the important coupling facility structures
    across the sites and still achieve the required performance. As discussed later in this chapter
    and illustrated in 
    <a href="#Table_2-1" class="links-within-text">Table 2-1</a>,
    the Coupling Link technology can support links
    upwards of 100 km with qualified dense wavelength division multiplexing (DWDM). However,
    this does not mean that your workload will tolerate even 1 km of distance between the z/OS
    images and the CF. Individual coupling operations will be delayed by 10 microseconds per
    kilometer. Although this time can be calculated, there is no safe way to predict the increased
    queuing effects caused by the increased response times and the degree of sharing that is
    unique to each environment. In other words, you will need to run your workload with
    connections at distance to evaluate the tolerance and impacts of distance.
  </p>
  <p>The benefits of a multisite workload come with more complexity. This must be taken into
    account when weighing the benefits of such configurations.
  </p>
  <h5><b>CF structure duplexing</b></h5>
  <p>Two mechanisms exist for duplexing CF structures. 
  </p>

    <ul>
      <li class="arrows">User-Managed Structure Duplexing is supported for use only with DB2 group buffer pool
        (GBP) structures. Duplexing the GBP structures can significantly reduce the time to
        recover the structures following a CF or CF connectivity failure. The performance impact of
        duplexing the GBP structures is small. Therefore, it is best to duplex the GBP structures
        used by a production DB2 data sharing group.
      </li>
      <li class="arrows">System-Managed Coupling Facility Structure Duplexing 
        referred to as SM duplexing provides a general purpose, hardware-assisted and easy-to-use mechanism for duplexing
        CF structures. This feature is primarily intended to allow installations to do data sharing
        without having to have a failure-isolated CF. However, the design of SM duplexing means
        that having the CFs a significant distance (kilometers) apart can have a dramatic impact
        on CF response times for the duplexed structures, and thus your applications, and needs
        careful planning and testing.
      </li>
    </ul>
  <p>In addition to the response time question, there is another consideration relating to the use of
    cross-site SM Duplexing. Because communication between the CFs is independent of the
    communication between mirrored disk subsystems, a failure that results in remote copy being
    suspended would not necessarily result in duplexing being suspended at the same instant. In
    case of a potential disaster, you want the data in the “remote” CF to be frozen in time at the
    same instant the “remote” disks are frozen, so you can restart your applications from the
    moment of failure.
  </p>
  <p>If you are using duplexed structures, it might seem that you are guaranteed to be able to use
    the duplexed instance of your structures if you must recover and restart your workload with
    the frozen secondary copy of your disks. However, this is not always the case. There can be
    rolling disaster scenarios where before, after, or during the freeze event, an interruption
    occurs (perhaps failure of CF duplexing links) that forces CFRM to drop out of duplexing.
    There is no guarantee that the structure instance in the surviving site is the one that will be
    kept. It is possible that CFRM keeps the instance in the site that is about to totally fail. In this
    case, there will not be an instance of the structure in the site that survives the failure.
  </p>
  <p>Furthermore, during a rolling disaster event, if you freeze secondary disks at a certain point
    but continue to update the primary disks and the CF structures, then the CF structures,
    whether duplexed or not, will not be usable if it is necessary to recover on the frozen
    secondary disks. This depends on some of your installation’s policies.
  </p>
  <p>To summarize, if there is a surviving, accessible instance of application-related structures,
    this might or might be consistent with the frozen secondary disks and therefore might or might
    not be usable. Furthermore, depending on the circumstances of the failure, even with
    structures duplexed across two sites, you are not 100% guaranteed to have a surviving,
    accessible instance of the application structures. Therefore, you must have procedures in
    place to restart your workloads without the structure contents.</p>
  <p>For more information, see the white paper titled <i>System-Managed CF Structure Duplexing</i>, 

   <!-- note that there was a link not working here called GM13-0103--> 
    <a href="">GM13-0103</a>
  </p>


  <h3 id="Data_consistency"><b>Data consistency</b></h3>
  <p>In an unplanned outage or disaster situation the ability to perform a database restart, rather
    than a database recovery, is essential to meet the recovery time objective (RTO) of many
    businesses, which typically are less than an hour. Database restart allows starting a database
    application (as you would follow a database manager abend or system abend) without having
    to restore it from backups. Database recovery is normally a process measured in many hours
    (especially if you have hundreds or thousands of databases to recover), and it involves
    restoring the last set of image copies and applying log changes to bring the databases up to
    the point of failure. 
  </p>
  <p>But, there is more to consider than simply the data for one data manager. What if you have an
    application that updates data in IMS, DB2, and VSAM? If you need to do a recover for these,
    will your recovery tools allow you to recover them to the same point in time and to the level of
    granularity that ensures that either all or none of the updates made by one transaction are
    recovered? Being able to do a restart rather than a recover avoids these issues.
  </p>
  <p>Data consistency across all copies of replicated data, spread across any number of storage
    subsystems, and in some cases across multiple sites, is essential to providing data integrity
    and the ability to perform a normal database restart if there is a disaster.
  </p>



  <h4 id="Dependent_write_logic"><b>Dependent write logic</b></h4>
    <p>Database applications commonly ensure the consistency of their data by using 
      <i>dependent
      write logic
    </i> regardless of whether data replication techniques are being used. Dependent
      write logic states that if 
      <code>I/O B</code> must logically follow 
      <code>I/O A</code>, so 
      <code>B</code> will not be started until 
      <code>A</code>
      completes successfully. This logic would normally be included in all software to manage data
      consistency. There are numerous instances within the software subsystem, such as
      databases, catalog/VTOC, and VSAM file updates, where dependent writes are issued.  
    </p>
    <p>As an example, <a href="#Figure_2_1" class="links-within-text">in Figure 2-1</a>, 
      LOG-P is the disk subsystem containing the database
      management system (DBMS) logs, and DB-P is the disk subsystem containing the DBMS
      data segments. When the DBMS updates a database, it also performs the following process:  
    </p>
      <ol>
        <li>
          1.  Write an entry to the log about the intent of the update.
        </li>
        <li>
          2.  Update the database.
        </li>
        <li>
          3.  Write another entry to the log indicating that the database was updated.
        </li>
      </ol>
    <p>
      If you will be doing a remote copy of these volumes, be sure that <i>all</i> of the updates are
      mirrored to the secondary disks.
    </p>


<!-- Screenshots to be taken from the pdf - so will NEED the real graphics at some point -->

    <div class="image_container">
      <figure>Graphic to go here
        <img src="" class=""
             alt="">
        <figcaption id="Figure_2_1"><em>Figure 2-1 Need for data consistency</em></figcaption>
    </figure>

    </div>

    <p>It is unlikely that all the components in a data center will fail at the same instant, even in the
      rare case of a full data center outage. The networks might fail first, or possibly one disk
      subsystem, or any other component in unpredictable combinations. No matter what happens,
      the remote image of the data must be managed so that cross-volume and subsystem data
      consistency is preserved during intermittent and staged failures that might occur over many
      seconds, even minutes. Such a staged failure is generally referred to as a <i>rolling disaster</i>.</p>
    <p>Data consistency during a rolling disaster is difficult to achieve for synchronous forms of
      remote copy because synchronous remote copy is entirely implemented within disk
      subsystem pairs. </p>
    <p>For example, in <a href="#Figure_2_1" class="links-within-text">in Figure 2-1</a> 
      the synchronously mirrored data sets are spread
      across multiple disk subsystems for optimal performance. The volume containing the DBMS
      log on the LOG-P disk subsystem in Site1 is mirrored to the secondary volume in the LOG-S
      disk subsystem in Site2, and the volume containing the data segments in the DB-P disk
      subsystem in Site1 is mirrored to the secondary volume in the DB-S disk subsystem in Site2. </p>
    <p>Assume that a disaster is in progress in Site1, causing the link between DB-P and DB-S to be
      lost before the link between LOG-P and LOG-S is lost. With the link between DB-P and DB-S
      lost, a write sequence of (1), (2), and (3) might be completed on the primary devices
      (depending on how the remote copy pair was defined) and the LOG writes (1) and (3) would
      be mirrored to the LOG-S device, but the DB write (2) would not have been mirrored to DB-S.
      A subsequent DBMS restart using the secondary copy of data in Site2 would clean up in-flight
      transactions and resolve in-doubt transactions, but the missing DB write (2) would not be
      detected. In this example of the missing DB write the DBMS integrity was compromised.
    </p>
    <aside><em>The way the disk subsystem reacts to a synchronous IBM Metro Mirror remote copy failure 
      depends on the options you specify when setting up the remote copy session. The behavior that 
      is described here is the default if no overrides are specified.</em></aside>
    <p>We discuss data consistency for synchronous remote copy in more detail in 
      <a href="#Metro_Mirror_data_consistency">Metro Mirror data consistency.</a></p>
    <p>For the two IBM asynchronous remote copy offerings, the consistency of the volumes in the
      recovery site is ensured because of the way these offerings work. This is described further in 
      <a href="#Global_Mirror">Global Mirror </a>and
      <a href="#XRC_data_consistency">XRC data consistency.</a>
    </p>
    <p>For GDPS Continuous Availability, which relies on asynchronous software replication as
      opposed to the use of Metro Mirror, XRC, or Global Mirror, consistency is managed within the
      replication software products. For more information, see 
      <a href="#IBM_software_replication_products">IBM software replication products.</a></p>


  <h3 id="Synchronous_versus_asynchronous_data_transfer"><b>Synchronous versus asynchronous data transfer</b></h3>
    <p><i>Synchronous</i> data transfer and <i>asynchronous</i> data transfer are two methods used to replicate
      data. Before selecting a data replication technology, you must understand the differences
      between the methods used and the business impact. 
    </p>

<!-- A formatted note -->

<div class="note-container">
  <p role="note" class="terminology">
    <strong>Terminology: </strong> In this book, we continue to use the term <i>Extended Remote Copy</i> (XRC)
    when referring to the asynchronous disk copy technology that is managed by the z/OS
    System Data Mover (SDM). The rebranded name of the IBM disk storage implementation
    is z/OS Global Mirror, which is used specifically when referring to the IBM implementation
    on the IBM Enterprise Storage Server and the IBM DS8000 family of products.
  </p>
  
</div>

<!-- END of formatted note -->

  <p>When using synchronous data transfer, as shown in 
    <a href="#Figure_2_-_2" class="links-within-text">Figure 2-2</a> by using IBM Metro Mirror, the
    application writes are first written to the primary disk subsystem (1) and then forwarded on to
    the secondary disk subsystem (2). When the data has been committed to <em>both</em> the primary
    and secondary disks (3), an acknowledgment that the write is complete (4) is sent to the
    application. Because the application must wait until it receives the acknowledgment before
    executing its next task, there will be a slight performance impact. Furthermore, as the
    distance between the primary and secondary disk subsystems increases, the write I/O
    response time increases because of signal latency.</p>
    <aside><em>Signal latency is related to the speed of light over fiber and is 10 microseconds per km, round trip.</em></aside>
  <p>The goals of synchronous replication are zero or near-zero loss of data, and quick recovery
    times from failures that occur at the primary site. Synchronous replication can be costly
    because it requires high-bandwidth connectivity. </p>
  <p>One other characteristic of synchronous replication is that it is an enabler for nondisruptive
    switching between the two copies of the data that is known to be identical. </p>



<!-- Screenshots to be taken from the pdf - so will NEED the real graphics at some point -->

<div class="image_container">
  <figure>Graphic to go here
    <img src="" class=""
         alt="">
    <figcaption id="Figure_2_-_2"><em>Figure 2-2 Synchronous versus asynchronous storage replication</em></figcaption>
</figure>

</div>

  <p>With asynchronous replication (see 
    <a href="#Figure_2_-_2">Figure 2-2</a>) with either XRC or Global Mirror, the
    application writes to the primary disk subsystem (1) and receives an acknowledgment that the
    I/O is complete as soon as the write is committed on the primary disk (2). The write to the
    secondary disk subsystem is completed in the background. Because applications do not have
    to wait for the completion of the I/O to the secondary device, asynchronous solutions can be
    used at virtually unlimited distances with negligible impact to application performance. In
    addition, asynchronous solutions do not require as much bandwidth as the synchronous
    solutions.</p>
  <p>With software-based asynchronous replication, as used in a GDPS Continuous Availability
    environment, the process is similar to that described for XRC. Data is captured from the
    database subsystem logs at the source copy when a transaction commits data to the
    database. That captured data is then sent asynchronously to a second location where it is
    applied to the target copy of the database in near real time.</p>
  <p>When selecting a data replication solution, perform a business impact analysis to determine
    which solution meets the businesses requirements while ensuring your service delivery
    objectives continue to be met; see 
    <a href="#Figure_2_-_3" class="links-within-text">Figure 2-3</a>. 
    The maximum amount of transaction loss that
    is acceptable to the business (RPO) is one measurement used to determine which remote
    copy technology should be deployed. If the business is able to tolerate loss of committed
    transactions, then an asynchronous solution will likely provide the most cost-effective
    solution. When no loss of committed transactions is the objective, then synchronous remote
    copy must be deployed. In this case, the distance between the primary and secondary remote
    copy disk subsystems, and the application’s ability to tolerate the increased response times,
    must be factored into the decision process.</p>

<!-- Screenshots to be taken from the pdf - so will NEED the real graphics at some point -->

<div class="image_container">
  <figure>Graphic to go here
    <img src="" class=""
         alt="">
    <figcaption id="Figure_2_-_3"><em>Figure 2-3 Business impact analysis</em></figcaption>
</figure>

</div>

<p>Many enterprises have both business and regulatory requirements to provide
  near-continuous data availability, without loss of transactional data, while protecting critical
  business data if there is a wide-scale disruption. This can be achieved by implementing
  three-copy (sometimes referred to as <i>3-site</i>) mirroring solutions that use both synchronous
  and asynchronous replication technologies. Synchronous solutions are used to protect
  against the day-to-day disruptions with no loss of transactional data. Asynchronous
  replication is used to provide out-of-region data protection, with some loss of committed data,
  for wide-spread disruptions. The key is to ensure cross-disk subsystem data integrity and
  data consistency is maintained through any type of disruption.</p>
<p>For more information about three-copy replication solutions see
  <a href="#Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery">Combining local and metro
    continuous availability with out of region disaster recovery.</a>
</p>


<h3 id="Data_replication_technologies"><b>Data replication technologies</b></h3>
<p>The two primary ways to make your data available following a disaster are as follows: 
</p>

<ul>
  <li class="arrows">By using a form of tape-based backup
  </li>
  <li class="arrows">By using data replication to a recovery site (also known as remote copy)</li>
    <p class="padding-left-para">This can be hardware-based or software-based replication.
    </p>
    
  
</ul>

<p>For companies with an RTO of a small number of hours or less, a tape-based solution is
  unlikely to be acceptable, because it is simply not possible to restore all your volumes and
  apply all database logs in the time available. Therefore, we are assuming that if you are
  reading this book you already have, or are planning to implement, some form of data
  replication technology. </p>
<p>Remotely copying your data eliminates the time that would be required to restore the data
  from tape and addresses the problem of having to recover data that is generated between the
  last backup of an application system and the time when the application system fails.
  Depending on the technology used, remote copy implementations provide a real-time (or near
  real-time) continuing copy of data between a source and a target.</p>
<p>IBM offers three basic technologies to provide this type of mirroring for disk storage:</p>

<ul>
  <li class="arrows">Metro Mirror: Updates to the primary volumes are synchronously mirrored to the remote
    volumes and all interactions related to this activity are done between the disk subsystems.
    Multi-target Metro Mirror (MTMM) is based on Metro Mirror and allows multiple secondary
    copies from the same primary
  </li>
  <li class="arrows">XRC: The task of retrieving the updates from the primary disk subsystem and applying
    those changes to the secondary volumes is done by a z/OS component named the
    System Data Mover (SDM).
  </li>
  <li class="arrows"> Global Mirror: This offering mirrors the data asynchronously. However, unlike XRC, all
    interactions are done between the disk subsystems rather than by an SDM. 
  </li>
</ul>



<!-- Appears to be a link missing in the pdf - see
  IBM System Storage DS8000: Copy Services for IBM System z, SG24-6787 -->

<p>These technologies are described more fully in the following sections. For an even more
  detailed explanation of the remote copy technologies described in the following sections, see
  <em>IBM System Storage DS8000: Copy Services for IBM System z</em>, SG24-6787.</p>
<p>IBM also offers several software-based replication products. Unlike the technologies listed for
  mirroring disk storage (which are application independent), most software replication
  products are specific to the database source and target in use. The following products are
  currently supported in a GDPS Continuous Availability environment:</p>

<ul>
  <li class="arrows">IBM InfoSphere Data Replication for IMS for z/OS
  </li>
  <li class="arrows">IBM InfoSphere Data Replication for VSAM for z/OS
  </li>
  <li class="arrows">IBM InfoSphere Data Replication for DB2 for z/OS 
  </li>
</ul>

<p>These products are introduced in the following sections. For more information, see 
  <a href="https://www.ibm.com/docs/en/products">IBM Knowledge Center.</a>
</p>


<h4 id="Metro_Mirror"><b>Metro Mirror</b></h4>
<p>Metro Mirror ensures that after the volume pair has been established and remains
  synchronized that the secondary volume will always contain exactly the same data as the
  primary. The IBM implementation of Metro Mirror provides synchronous data mirroring at
  distances up to 300 km (and potentially even greater distances, after technical review and
  approval).  
</p>

<!-- A formatted IMPORTANT note -->

<div class="note-container">
  <p role="note" class="important-note">
    <strong>Important: </strong>Always use caution when considering long distances. When we say that
    something is “supported up to xx km,” it means that the technology will work at that
    distance if you have qualified cross-site connectivity technology that supports that protocol.
    See
    <a href="#Cross_-_site_connectivity_considerations">Cross-site connectivity considerations
    </a>for more details.
  <br>
  <br>
    You must also consider the impact the increased response time will have on your
    applications. Some applications can tolerate the response time increase associated with
    cross-site distances of 100 km, but the same distance in another installation might make it
    impossible for the applications to deliver acceptable levels of performance.  
    <br>
    <br>
    So, carefully evaluate the projected response time impact, and apply that increase to your
    environment to see if the result is acceptable. Your vendor storage specialist can help you
    determine the disk response time impact of the proposed configuration.
  </p>
</div>
<h5><b>Recovery point objective with Metro Mirror</b></h5>
<p>If you have a recovery point objective of zero, meaning zero data loss, Metro Mirror is the only
  IBM remote copy option that can achieve that objective.
</p>
<p>That is not to say that you will always have zero data loss if using Metro Mirror. Zero data loss
  means that there will never be updates made to the primary disks that are not mirrored to the
  secondaries. The only way to ensure that zero data loss is to immediately stop all update
  activity to the primary disks if the remote copy relationship ceases to exist (if you lose
  connectivity between the primary and secondary devices, for example).
</p>
<p>Thus, choosing to have zero data loss really means that you must have automation in place
  that will stop all update activity in the appropriate circumstances. It also means that you
  accept the possibility that the systems can be stopped for a reason other than a real disaster;
  for example, if the failure was caused by a broken remote copy link rather than a fire in the
  computer room. Completely avoiding single points of failure in your remote copy
  configuration, however, can reduce the likelihood of such events to an acceptably low level.
</p>
<h5><b>Supported platforms with Metro Mirror</b></h5>
<p>Metro Mirror replication is supported for any IBM or non-IBM disk subsystem that supports
  the Metro Mirror architecture, specifically the Freeze/Run capability. Metro Mirror can mirror
  fixed-block (FB) devices that are used by IBM Z and platforms other than IBM Z and CKD
  devices that are used by mainframe operating systems, such as IBM z/OS, IBM z/VM, and
  IBM z/VSE. 
</p>

<!-- Chapter 3 full content is not yet all in -->

<p>Not all operating systems necessarily support an interface to control the remote copy
  function. However, the Metro Mirror function for FB devices can be controlled from a
  connected z/OS system if the disk storage subsystem supports the zFBA feature (as
  described in 
  <a href="#FB_disk_management_prerequisites_for_GDPS_Metro">FB disk management prerequisites</a> for GDPS Metro, and in
  <a href="#FB_disk_management_prerequisites_for_GDPS_Metro_HyperSwap_Manager">FB disk management prerequisites</a> for GDPS Metro HyperSwap Manager).
</p>
<p>With current implementations of Metro Mirror, the primary and secondary disk subsystems
  must be from the same vendor, although vendors (including IBM) often support Metro Mirror
  between different disk subsystem models of their own product lines. This can help with
  migrations and technology upgrades.
</p>
<h5><b>Distance with Metro Mirror</b></h5>
<p>The maximum distance supported for IBM Metro Mirror is 300 km (without an RPQ). Typical
  GDPS Metro and GDPS Metro HyperSwap Manager configurations are limited to distances
  less than this because of Coupling Link configurations. For more information about the
  supported distances for these Parallel Sysplex connections, see
  <a href="#Coupling_links">Coupling links.</a>  You will also need to contact other storage vendors to understand the maximum
  distances supported by their Metro Mirror compatible mirroring implementations.
</p>
<h5><b>Performance with Metro Mirror</b></h5>
<p>As the distance between your primary and secondary disk subsystems increases, the time it
  takes for your data to travel between the subsystems also increases. This might have a
  performance impact on your applications because they cannot proceed until the write to the
  secondary device completes. 
</p>
<p>Be aware that as response times increase, link use also increases. Depending on the type
  and number of Metro Mirror links you configured, more links and the use of Parallel Access
  Volumes (PAVs) might help to provide improved response times at longer distances.</p>
<p>Disk Magic, a tool available to your IBM storage specialist, can be used to predict the impact
  of various distances, link types, and link numbers for IBM disk implementation. We consider
  access to the information provided by such a tool essential to a GDPS project using Metro
  Mirror.</p>
<h5><b>Metro Mirror connectivity</b></h5>
  <p>Connectivity between the primary and secondary disk subsystems can be provided by direct
    connections between the primary and secondary disk subsystems, by IBM FICON switches,
    by DWDMs, and by channel extenders.  
  </p>
  <p>The type of intersite connection (dark fiber or telecommunications link) available determines
    the type of connectivity you use: telecommunication links can be used by channel extenders,
    and the other types of connectivity require dark fiber.</p>
  
  <!-- Appears to be a link in pdf but not working there -->
  
    <p>For more information about connectivity options and considerations for IBM Z, see the most
    recent version of <em>IBM System z Connectivity Handbook</em>, SG24-5444.</p>
<h5 id="Metro_Mirror_data_consistency"><b>Metro Mirror data consistency</b></h5>
  <p>When using Metro Mirror, the following sequence of actions occurs when an update I/O is issued to a primary volume:  
  </p>
  <ol>
    <li>
      1. Write to the primary volume (disk subsystem cache and non-volatile store (NVS)).
      <p class="padding-left-para">
        Your production system writes data to a primary volume and a cache hit occurs.
      </p>
      
    </li>
    <li>
      2. Write to the secondary (disk subsystems cache and NVS).
      <p class="padding-left-para">
        The primary disk subsystem’s microcode then sends the update to the secondary disk subsystem’s cache and NVS.
      </p>
      
    </li>
    <li>
      3. Signal write is complete on the secondary.
      <p class="padding-left-para">
        The secondary disk subsystem signals write complete to the primary disk subsystem when the 
        updated data is in its cache and NVS.
      </p>
      
    </li>
    <li>
      4. Post I/O is complete. 
      <p class="padding-left-para">
        When the primary disk subsystem receives the write complete from the 
      secondary disk subsystem, it returns 
      <code>Device End (DE)</code> 
      status to your application program. 
      Now, the application program can continue its processing and move on to any dependent writes that might 
      have been waiting for this one to complete.
      </p>
      
    </li>
  </ol>
  <p>
    However, Metro Mirror on its own provides this consistency only for a single write. 
    Guaranteeing consistency across multiple logical subsystems and even across multiple disk 
    subsystems requires automation on top of the Metro Mirror function. 
    This is where GDPS comes in with freeze automation, which is described in the following sections:
  </p>

  <ul>
    <li class="arrows">
      <p>
        <a href="#Protecting_data_integrity_and_data_availability_with_GDPS_Metro">
          Protecting data integrity and data availability with GDPS Metro</a> for GDPS Metro.
      </p>
    </li>
    <li class="arrows">
      <p>
        <a href="#Protecting_data_integrity_and_data_availability_with_GDPS_HM">
          Protecting data integrity and data availability with GDPS HM</a> 
          for GDPS Metro HyperSwap Manager.
      </p>
     
    </li>
  </ul>


<h5><b>Metro Mirror transparent disk swap</b></h5>
  <p>Because under normal conditions the primary and secondary disks are known to be identical, 
    with Metro Mirror it is possible to swap to using the secondary copy of the disks in a manner 
    that is transparent to applications that are using those disks. This task is not simple. 
    It requires tight control and coordination across many devices that are shared by multiple systems 
    in a timely manner. GDPS Metro and GDPS Metro HyperSwap Manager automation, with support provided in z/OS, 
    z/VM, and specific distributions of Linux on IBM Z, provide such a transparent swap capability and it is known as 
    <i>HyperSwap</i>. 
  </p>
  <p>HyperSwap is a key availability-enabling technology. For more information about GDPS HyperSwap, see the following sections:

  </p>
 
    <ul>
      <li class="arrows">
        <p>
          <a href="#GDPS_HyperSwap_function_chapter3">
            GDPS HyperSwap function</a> for GDPS Metro.
        </p>

      </li>
      <li class="arrows">
        <p>
          <a href="#GDPS_HyperSwap_function_chapter4">
            GDPS HyperSwap function
          </a> for GDPS Metro HyperSwap Manager.
            
        </p>

      </li>
      <li class="arrows">
        <p>
          <a href="#GDPS_HyperSwap_function_chapter8">
            GDPS HyperSwap function
          </a> for the GDPS Virtual Appliance.
        </p>

      </li>
    </ul>
  <h5><b>Addressing z/OS device limits in a GDPS Metro environment</b></h5>
  <p>As clients implement IT resiliency solutions that rely on multiple copies of data, 
    more are finding that the z/OS limit of 64K (65,536) devices is limiting their ability 
    to grow or even to take advantage of technologies like HyperSwap. 
    Clients can consolidate data sets to fewer larger volumes, but even with that, 
    there are times when this might not make operational sense for all types of data.
  </p>
  <p>As a result, z/OS introduced the concept of an “alternate subchannel set,” which can include the definition 
    for certain types of disk devices. 
    An alternate subchannel set provides another set of 64K devices for the following device types:
  </p>
 

  <ul>
    <li class="arrows">
      Parallel Access Volume (PAV) alias devices
    </li>
    <li class="arrows">
      Metro Mirror secondary devices (defined as 3390D)
    </li>
    <li class="arrows">
      FlashCopy target devices
    </li>
  </ul>


  <p>Including PAV alias devices in an alternate subchannel set is transparent to GDPS and 
    is common practice for current GDPS Metro HyperSwap Manager and GDPS Metro environments.

  </p>
  <p>Support is included in GDPS Metro HyperSwap Manager and GDPS Metro to allow definition of 
    Metro Mirror secondary devices in an alternate subchannel set. With this feature, GDPS can 
    support Metro Mirror configurations with nearly 64K device pairs. GDPS Metro HyperSwap Manager 
    allows the secondary devices for z/OS systems in the GDPS sysplex, as well as for managed z/VM systems 
    (and guests) to be defined in an alternate subchannel set. GDPS Metro only supports alternate 
    subchannel sets for z/OS systems in the sysplex.
  </p>
  <p>There are limitations to keep in mind when considering the use of this feature. 
    Specifically, enhanced support is provided in IBM zEnterprise 196 or 114 servers that allow 
    the Metro Mirror secondary copy of the IPL, IODF, and stand-alone dump devices for z/OS systems 
    in the GDPS sysplex to also be defined in the alternate subsystem set (MSS1).
  </p>
  <p>With this support, a client can define all z/OS Metro Mirrored devices belonging to the 
    GDPS sysplex uniformly with their secondary in the alternate subchannel set. This removes the 
    necessity to define IPL, IODF, and stand-alone dump devices differently in MSS0.
  </p>
  <p>The use of alternate subchannel sets for the FlashCopy target devices that are managed 
    by GDPS is not necessary because no requirement exists to define UCBs for these devices 
    (they can be in any subchannel set or not defined at all). This issue contributes to the 
    ability of GDPS to support Metro Mirror configurations with nearly 64 K device pairs because 
    no device numbers or UCBs are used by the FlashCopy target devices.
  </p>

<h5><b>Multi-Target Metro Mirror</b></h5>

<!-- A formatted note -->

<div class="note-container">
  <p role="note" class="info_repeated_multiple_chapters">
    <strong>Note: </strong>There is no requirement to define UCBs for the FlashCopy target devices that are managed by GDPS.
  </p>
</div>
<p>Multi-target PPRC, also known as MT-PPRC, is based on the PPRC (Metro Mirror) technology. 
  The MT-PPRC architecture allows multiple secondary, synchronous, or asynchronous secondary 
  devices for a single primary device.
</p>
<p>Multi-Target Metro Mirror (MTMM) is a specific topology that is based on the MT-PPRC technology, 
  which allows maintaining two synchronous Metro Mirror secondary targets (two Metro Mirror legs) 
  from a single primary device. Each leg is tracked and managed independently. Consider the following points:
</p>
<ul>
  <li class="arrows">
    Data is transferred to both targets in parallel.
  </li>
  <li class="arrows">
    Pairs operate independent of each other.
  </li>
  <li class="arrows">
    Pairs may be established, suspended or removed separately.
  </li>
  <li class="arrows">
    A replication problem on one leg does not affect the other leg.
  </li>
  <li class="arrows">
    HyperSwap is possible on either leg.
  </li>
</ul>
<p>
  MTMM provides all the benefits of Metro Mirror plus has the extra protection of a second synchronous leg.
</p>
<h5><b>Summary</b></h5>
  <p>
    Metro Mirror synchronous replication gives you the ability to remote copy your data in real time, 
    with the potential for no data loss at the recovery site. Metro Mirror is your only choice if your RPO is zero. 
    Metro Mirror is the underlying remote copy capability that the GDPS Metro, GDPS Metro HyperSwap Manager, 
    and GDPS Virtual Appliance offerings are built on.
  </p>

<h4 id="XRC(z/OS_Global_Mirror)"><b>XRC (z/OS Global Mirror)</b></h4>
<p>The Extended Remote Copy (XRC) solution consists of a combination of software and hardware functions. 
  XRC maintains a copy of the data asynchronously at a remote location. It involves a System Data Mover (SDM) 
  that is a component of the z/OS operating system working with supporting microcode in the primary disk subsystems. 
  One or more SDMs running in the remote location are channel-attached to the primary disk subsystems. 
  They periodically pull the updates from the primary disks, sort them in time stamp order, and apply 
  the updates to the secondary disks. This provides point-in-time consistency for the secondary disks. 
  The IBM implementation of XRC is branded as z/OS Global Mirror. This name is used interchangeably 
  with XRC in many places, including in this book.  
</p>
<h5><b>Recovery point objective</b></h5>
  <p>
    Because XRC collects the updates from the primary disk subsystem some time after the I/O has completed, 
    there will always be an amount of data that has not been collected when a disaster hits. As a result, 
    XRC can be used only when your recovery point objective is greater than zero (0). 
    The amount of time that the secondary volumes lag behind the primary depends mainly on the following items:
  </p>
  <ul>
    <li class="arrows">The performance of the SDM</li>
      <p class="padding-left-para">The SDM is responsible for collecting, sorting, and applying all updates. 
        If insufficient capacity (MIPS, storage, and I/O resources) is available to the SDM, 
        longer delays collecting the updates from the primary disk subsystems will occur, causing 
        the secondaries to drift further behind during peak times.
      </p>
      
    
    <li class="arrows">The amount of bandwidth</li>
      <p class="padding-left-para">
        If there is insufficient bandwidth to transmit the updates in a timely manner, contention 
        on the remote copy links can cause the secondary volumes to drift further behind at peak times.
      </p>      
      
    
    <li class="arrows">The use of device blocking</li>
      <p class="padding-left-para">
        Enabling blocking for devices results in I/O write activity to be paused for devices with high update rates. 
        This allows the SDM to offload the write I/Os from cache, resulting in a smaller RPO.
      </p>

   
    <li class="arrows">The use of write pacing</li>
      <p class="padding-left-para">
        Enabling write pacing for devices with high write rates results in delays being inserted into the application’s 
        I/O response to prevent the secondary disk from falling behind. This option slows the I/O activity, resulting in 
        a smaller RPO; it is less disruptive than device blocking. Write pacing, if wanted, can be used in conjunction 
        with the z/OS Workload Manager (WLM).
      </p>

  </ul>
  <p>
    Because XRC is able to pace the production writes, it is possible to provide an average RPO of 1 to 
    5 seconds and maintain a guaranteed maximum RPO, if sufficient bandwidth and resources are available. 
    However, it is possible that the mirror will suspend, or that production workloads will be impacted, 
    if the capability of the replication environment is exceeded because of either of the following reasons:
  </p>
    <ul>
      <li class="arrows">
        Unexpected peaks in the workload
      </li>
      <li class="arrows">
        An underconfigured environment
      </li>
      

    </ul>
  <p>
    To minimize the lag between the primary and secondary devices, you must have sufficient connectivity and a 
    well-configured SDM environment. For more information about planning for the performance aspects of your 
    XRC configuration, see the chapter about capacity planning in <em>DFSMS Extended Remote Copy Installation 
      Planning Guide</em>, GC35-0481.
  </p>

<h5><b>Supported platforms</b></h5>
  <p>
    There are two aspects to “support” for XRC. The first aspect is the ability to append a time stamp to all 
    write I/Os so the update can subsequently be remotely copied by an SDM. This capability is provided in the 
    following operating systems:
  </p>
    <ul>
      <li class="arrows">
        Any supported release of z/OS
      </li>
      <li class="arrows">
        Linux on IBM Z when using CKD format disks
      </li>
      <li class="arrows">
        z/VM with STP and appropriate updates (contact IBM support for the more information)
      </li>
    </ul>

<!-- A formatted note -->

<div class="note-container">
  <p role="note" class="info_repeated_multiple_chapters">
    <strong>Note: </strong>XRC does <em>not</em> support FB devices.
  </p>
</div>
  <p>
    It is also possible to use XRC to remote copy volumes that are being used by IBM Z operating systems 
    that do <em>not</em> time stamp their I/Os. However, in this case, it is not possible to provide consistency 
    across multiple LSSs. The devices must all be in the same LSS to provide consistency. For more 
    information, see the section about understanding the importance of timestamped writes in the 
    most recent revision of <em>z/OS DFSMS Advanced Copy Services</em> manual.
  </p>
  <p>
    The other aspect is which systems can run the System Data Mover function. 
    In this case, the only system that supports this is any supported release of z/OS.
  </p>

<h5><b>Distance and performance</b></h5>
  <p>
    Because XRC is an asynchronous remote copy capability, the amount of time it takes to mirror the update 
    to the remote disks does not affect the response times to the primary volumes. As a result, 
    virtually unlimited distances between the primary and secondary disk subsystems are supported, 
    with minimal impact to the response time of the primary devices.
  </p>
<h5><b>Connectivity</b></h5>
  <p>
    If the recovery site is within the distance supported by a direct FICON connection, 
    switches/directors, or DWDM, then you can use one of these methods to connect the SDM system to 
    the primary disk subsystem. Otherwise, you must use channel extenders and telecommunication lines.
  </p>
<h5 id="XRC_data_consistency"><b>XRC data consistency</b></h5>
  <p>
    XRC uses time stamps and consistency groups to ensure that your data is consistent across the copy operation. 
    When an XRC pair is established, the primary disk subsystem notifies all systems with a logical path group for 
    that device, and the host system DFSMSdfp software starts to time stamp all write I/Os to the primary volumes. 
    This is necessary to provide data consistency.
  </p>
  <p>
    XRC is implemented in a cooperative way between the disk subsystems in the primary site and the SDMs, 
    which typically are in the recovery site. The data flow includes the following process (see 
    <a href="#Figure_2_-_4">Figure 2-4</a>):
  </p>
    <ol>
      <li>1. The primary system writes to the primary volumes.
      </li>
      <li>2. Primary disk subsystem posts I/O complete.</li>
       <p class="ol-list-padding-left-para">Your application I/O is signaled completed when the data is written to the primary disk subsystem's 
        cache and NVS. 
        <code>Channel End (CE)</code> and 
        <code>Device End (DE)</code> are returned to the writing application. 
        These signal that the updates have completed successfully. A time stamped copy of the update is 
        kept in the primary disk subsystems cache. Dependent writes can proceed now.
      </p>
    </ol>

<!-- Screenshot to be taken from the pdf - so will NEED the real graphic at some point -->

<div class="image_container">
  <figure>Graphic to go here
    <img src="" class=""
         alt="">
    <figcaption id="Figure_2_-_4"><em>Figure 2-4 Data flow when using z/OS Global Mirror</em></figcaption>
</figure>

</div>
    <ol>
      <li>3. Offload data from primary disk subsystem to SDM.</li>
      <p class="ol-list-padding-left-para">
        Every so often (several times a second), the SDM requests each of the primary disk subsystems 
        to send any updates that have been received. The updates are grouped into record sets, which are 
        asynchronously offloaded from the cache to the SDM system.
      </p>
      <p class="ol-list-padding-left-para">
        Within the SDM, the record sets, perhaps from multiple primary disk subsystems, are processed 
        into consistency groups (CGs) by the SDM. The CG contains records that have their order of 
        update preserved across multiple disk subsystems participating in the same XRC session. 
        This preservation of order is vital for dependent write I/Os such as databases and logs. 
        The creation of CGs guarantees that XRC applies the updates to the secondary volumes 
        with update sequence integrity for any type of data.
      </p>
      <li>4. Write to secondary.</li>
      <p class="ol-list-padding-left-para">  
        When a CG is formed, it is written from the SDM’s buffers to the SDM’s journal data sets. 
        Immediately after the CG has been hardened on the journal data sets, the records are written to 
        their corresponding secondary volumes. Those records are also written from the SDM’s buffers.
      </p>
      <li>5. The XRC control data set is updated to reflect that the records in the CG have been written 
        to the secondary volumes.
      </li>
    </ol>
  <h5 id="Coupled_Extended_Remote_Copy"><b><em>Coupled Extended Remote Copy</em></b>
  </h5>
    <p>XRC is an effective solution for mirroring many thousands of volumes. However, a single SDM instance 
      can manage replication only for a finite number of devices. You can use the Coupled XRC (CXRC) 
      support to extend the number of devices for added scalability.
    </p>
    <p>CXRC provides the ability to couple multiple SDMs running in the same or different LPARs together into 
      a <i>master</i> session. CXRC coordinates the consistency of data across coupled sessions in a master session, 
      allowing recovery of data for all the volumes in the coupled sessions to a consistent time.
    </p>
    <p>If the sessions are not coupled, recoverable consistency is provided only within each individual SDM, 
      not across SDMs. All logically related data (for example, all the data used by a single sysplex) 
      should be copied by one SDM, or a single group of coupled SDMs.
    </p>
  <h5 id="Multiple_Extended_Remote_Copy"><b><em>Multiple Extended Remote Copy</em></b>
  </h5>
    <p>In addition to the additional capacity enabled by Coupled XRC, there is also an option called 
      Multiple XRC (MXRC). MXRC allows you to have up to 20 SDMs in a single LPAR, of which 13 can be coupled 
      together into a cluster. These can then be coupled to SDMs or clusters running in other LPARs through 
      CXRC. Up to 14 SDM clusters can then be coupled together, allowing for an architectural limit of 
      coupled consistency across 182 SDMs.
    </p>    
  <h5><b><em>Multiple Reader</em></b>
  </h5>

    <p>XRC Multiple Reader (also known as Extended Reader) allows automatic load balancing over multiple 
      readers in an XRC environment. A <i>reader</i> is a task that is responsible for reading updates from a 
      primary LSS. Depending on the update rate for the disks in an LSS, a reader task might not be 
      able to keep up with pulling these updates and XRC could fall behind. The function can provide 
      increased parallelism through multiple SDM readers and improved throughput for XRC remote 
      mirroring configurations.
    </p>
    <p>It can allow XRC to do these tasks:
    </p>

    <ul>
      <li class="arrows">
        Better sustain peak workloads for a given bandwidth
      </li>
      <li class="arrows">
        Increase data currency over long distances
      </li>
      <li class="arrows">
        Replicate more capacity while maintaining the same recovery point objective
      </li>
      <li class="arrows">
        Help avoid potential slowdowns or suspends caused by I/Os that are not being processed fast enough
      </li>
    </ul>
    <p>Before the introduction of Multiple Readers, you needed to plan carefully to balance the primary volume 
      update rate versus the rate at which the SDM could “drain” the data. If the drain rate was unable to 
      keep up with the update rate, there was a potential to affect application I/O performance.
    </p>
    <p>GDPS XRC can use this multireader function, and thus provide these benefits.
    </p>

  <h5 id="Extended_Distance_FICON"><b><em>Extended Distance FICON</em></b>
  </h5>
    <p>Extended Distance FICON is an improvement focused on providing XRC clients a choice of selecting 
      less complex channel extenders built on frame forwarding technology rather than channel extenders 
      that need to emulate XRC read commands to optimize the channel transfer through the channel extender 
      to get the best performance.
    </p>
    <p>Extended distance FICON enables mirroring over longer distances without substantial reduction of 
      effective data rate. It can significantly reduce the cost of remote mirroring over FICON for XRC.
    </p>
    <p>Extended Distance FICON is supported only on the IBM Z10 and later servers, and the IBM System 
      Storage DS8000 disk subsystems.
    </p>
  <h5><b><em>SDM offload to zIIP</em></b>
  </h5>
    <p>
      The System Data Mover (SDM) is allowed to run on one of the specialty engines that are referred 
      to as a IBM Z Integrated Information Processor (zIIP), which are offered on IBM Z9
      and later processors. By offloading some of the SDM workload to a zIIP, better price performance 
      and improved use of resources at the mirrored site can be achieved.
    </p>
    <p>
      One benefit is that DFSMS SDM processing is redirected to a zIIP processor, which can lower server 
      use at the mirrored site. Another benefit is that with an investment of a zIIP specialty processor 
      at the mirrored site, you might now be able to cost-justify the investment in and implementation 
      of a disaster recovery solution that can lower server use at the mirrored site, while at the same 
      time reduce software and hardware fees.
    </p>
  <h5><b>Scalability in a GDPS XRC environment</b>
  </h5>
    <p>
      As clients implement IT resiliency solutions that rely on multiple copies of data, more are finding 
      that the z/OS limit of 64K (65,536) devices is limiting their ability to grow. Clients can consolidate 
      data sets to fewer larger volumes, but even with that, there are times when this might not make operational 
      sense for all types of data.
    </p>
    <p>
      In an XRC replication environment, the SDM system or systems are responsible for performing replication. 
      An SDM system will need to address a small number of XRC infrastructure volumes plus the primary 
      and secondary XRC devices that it is responsible for and possibly the FlashCopy target devices. 
      This means that each SDM system can manage XRC replication for up to roughly 21K primary devices, 
      assuming target FlashCopy devices are also defined to the SDM system. However, as described 
      in
      <a href="#Multiple_Extended_Remote_Copy">Multiple Extended Remote Copy</a> and 
      <a href="#Coupled_Extended_Remote_Copy">Coupled Extended Remote Copy</a>, 
      it is possible to run multiple clustered and coupled SDMs across multiple z/OS images. 
      As you can see, you have more than ample scalability.
    </p>
    <p>
      Also, it is possible in a GDPS XRC environment to use “no UCB” FlashCopy, in which case you do not need to 
      define the FlashCopy target devices to the SDM systems. This configuration further increases the number of 
      devices each SDM system can handle.
    </p>
    <p>
      However, UCBs for the FlashCopy target devices must be defined in a separate LPAR somewhere in the environment 
      to bring these devices online and perform an XRC recover operation.
    </p>
    <p>
      Another option is to define the FlashCopy target devices in MSS2 in the SDM systems. GDPS provides the 
      capability to swap the FlashCopy target devices that are defined in MSS2 into the active subchannel 
      set on the controlling system and then, perform an XRC recover operation there. This process 
      eliminates the need for a special LPAR with addressibility to the FlashCopy target devices 
      to perform the XRC recover while still maximizing the number of devices that each SDM system can manage.
    </p>
  <h5><b>Hardware prerequisites</b>
  </h5>
    <p>
      XRC requires, on IBM disk subsystems, that primary IBM disk subsystems have the IBM z/OS Global Mirror 
      feature code installed. It is not necessary for the primary and secondary disks to be the same device type, 
      although they must both have the same geometry and the secondary device must be at least as large 
      as the primary device.
    </p>
    <p>
      XRC is also supported on disk subsystems from other vendors that have licensed and implemented the 
      interfaces from IBM, and it is possible to run with a heterogeneous environment with multiple 
      vendors’ disks. Target XRC volumes can also be from any vendor, even if the target subsystem 
      does not support XRC, thus enabling investment protection.
    </p>


<!-- A formatted note -->

<div class="note-container">
  <p role="note" class="info_repeated_multiple_chapters">
    <strong>Note: </strong>
    
      Keep in mind that at some point, you might have to remote copy from the 
      recovery site back to the production site. GDPS XRC defines procedures and provides 
      specific facilities for switching your production workload between the two regions.
    
    <br>
    <br>
    
      To reverse the XRC direction, the IBM z/OS Global Mirror feature code must also be 
      installed in the secondary disk subsystems that will become primary when you reverse 
      the replication direction. To reverse the replication direction, the primary and secondary devices must be the same size.

    <br>
    <br>
      In summary, it makes sense to maintain a symmetrical configuration across both primary and secondary devices.
    
    
  </p>
</div>


    <p>
      An extra requirement is that all the systems writing to the primary volumes must be connected to the 
      same STP network. It is not necessary for them all to be in the same <i>sysplex</i>, simply 
      that they all share the same <i>time source</i>. 
    </p>
  <h5><b>Summary</b>
  </h5>
    <p>
      XRC offers a proven disk mirroring foundation for an enterprise disaster recovery solution 
      that provides large scalability and good performance.  
    </p>
    <p>
      XRC is a preferred solution if your site has these requirements:
    </p>
    <ul>
      <li class="arrows">
        Extended distances between primary and recovery site
      </li>
      <li class="arrows">
        Consistent data, at all times, in the recovery site
      </li>
      <li class="arrows">
        Ability to maintain the highest levels of performance on the primary system
      </li>
      <li class="arrows">
        Can accept a small time gap between writes on the primary system and the subsequent mirroring 
        of those updates on the recovery system
      </li>
      <li class="arrows">
        Scale with performance to replicate a large number of devices with consistency
      </li>
      <li class="arrows">
        Run with a heterogeneous environment with multiple vendors’ disks
      </li>
    </ul>

  <h4 id="Global_Mirror"><b>Global Mirror</b></h4>
    <p>
      Global Mirror is an asynchronous remote copy technology that enables a 2-site disaster recovery 
      and backup solution for the IBM Z and distributed systems environments. Using asynchronous technology, 
      Global Mirror operates over Fibre Channel Protocol (FCP) communication links and maintains a consistent 
      and restartable copy of data at a remote site that can be located at virtually unlimited distances 
      from the local site.  
    </p>
    <p>
      Global Mirror works by using three sets of disks, as shown in 
      <a href="#Figure_2_-_5">Figure 2-5</a>. Global Copy 
      (PPRC Extended Distance, or PPRC-XD), which is an asynchronous form of PPRC (Metro Mirror), 
      is used to continually transmit data from the primary (A) to secondary (B) volumes, using the 
      out-of-sync bitmap to determine what needs to be transmitted. Global Copy does not guarantee 
      that the arriving writes at the local site are applied to the remote site in the same sequence. 
      Therefore, Global Copy by itself does not provide data consistency.  
    </p>
    <p>  
      If there are multiple physical primary disk subsystems, one of them is designated as the <i>Master</i> 
      and is responsible for coordinating the creation of consistency groups. The other disk subsystems 
      are subordinates to this Master.
    </p>
    <p> 
      Each primary device maintains two bitmaps. One bitmap tracks incoming changes. The other bitmap 
      tracks which data tracks must be sent to the secondary before a consistency group can be formed 
      in the secondary. 
    </p>
    <p>  
      Periodically, depending on how frequently you want to create consistency groups, the Master disk 
      subsystem will signal the subordinates to pause application writes and swap the change recording bitmaps. 
      This identifies the bitmap for the next consistency group. While the I/Os are paused in all LSSs in 
      the Global Mirror session, any dependent writes will not be issued because the CE/DE has not been returned. 
      This maintains consistency across disk subsystems. The design point to form consistency groups is 2 - 3 ms.
    </p>
    <p>  
      After the change recording bitmaps are swapped, write I/Os are resumed and the updates that remain on the 
      Global Mirror primary for the current consistency group will be drained to the secondaries. After all of 
      the primary devices have been drained, a FlashCopy command is sent to the Global Mirror secondaries (B), 
      which are also the FlashCopy source volumes, to perform a FlashCopy to the associated FlashCopy target 
      volumes (C). The tertiary or C copy is a consistent copy of the data.
    </p>
    <p>  
      Remember, the B volumes are secondaries to Global Copy and are not guaranteed to be consistent. 
      The C copy provides a “golden copy” which can be used to make the B volumes consistent in case recovery 
      is required. Immediately after the FlashCopy process is logically complete, the primary disk subsystems 
      are notified to continue with the Global Copy process. For more information about 
      FlashCopy, see
      <a href="#FlashCopy">FlashCopy</a>.
    </p>
    <p>  
      After Global Copy is resumed, the secondary or B volumes are inconsistent. However, if recovery is needed, 
      the FlashCopy target volumes provide the consistent data for recovery.
    </p>
    <p>  
      All this processing is done under the control of microcode in the disk subsystems. You can have up to 16 mirrored 
      pairs in a pool, one of which is the Master primary and secondary pair, (see
      <a href="#Figure_2_-_5" class="links-within-text">Figure 2-5).</a>
    </p>

    <!-- Screenshot taken from the pdf - so will NEED the real graphic at some point -->

    <div class="image_container">
      <figure>GRAPHIC TO BE PLACED HERE
        <img src="" class=""
             alt="">
        <figcaption id="Figure_2_-_5"><em>Figure 2-5 Global Mirror: How it works</em></figcaption>
    </figure>

    </div>

    <h5><b>Recovery point objective</b>
    </h5>
      <p>
        Because Global Mirror is an asynchronous remote copy solution, there will always be an 
        amount of data that must be re-created following a disaster. As a result, Global Mirror 
        can be used only when your recovery point objective (RPO) requirement is greater than zero 
        (0). The amount of time that the FlashCopy target volumes lag behind the primary depends 
        mainly on the following items:
      </p>
      <ul>
        <li class="arrows">How often consistency groups are built</li>
          <P class="padding-left-para">
            This is controlled by the installation and can be specified in terms of seconds.
          </P>
          
        
        <li class="arrows">The amount of bandwidth</li>
          <P class="padding-left-para">
            If there is insufficient bandwidth to transmit the updates in a timely manner, contention on the 
            remote copy links can cause the secondary volumes to drift further behind at peak times. The more 
            frequently you create consistency groups, the more bandwidth you will require.
          </P>

      </ul>
      <p>
        Although it is not unusual to have an average RPO of 5 - 10 seconds with Global Mirror, 
        it is possible that the RPO will increase significantly if production write rates exceed the 
        available resources. However, unlike z/OS Global Mirror, the mirroring session will not be 
        suspended and the production workload will not be impacted if the capacity of the replication 
        environment is exceeded because of unexpected peaks in the workload or an underconfigured environment.
      </p>
      <p>
        To maintain a consistent lag between the primary and secondary disk subsystems, you must have 
        sufficient connectivity. For more information about planning for the performance aspects of your 
        Global Mirror configuration, see <em>IBM DS8870 Copy Services for IBM z Systems</em>, SG24-6787.
      </p>
    <h5><b>Supported platforms</b>
    </h5>
      <p>
        The IBM Enterprise Storage Server and DS8000 families of disk subsystems support Global Mirror. 
        For other enterprise disk vendors, contact your vendor to determine whether they support Global Mirror 
        and if so, on which models.  
      </p>
    <h5><b>Distance and connectivity</b>
    </h5>
      <p>
        Because Global Mirror is an asynchronous remote copy capability, the amount of time it takes to mirror the 
        update to the remote disks does not affect the response times to the primary volumes. As a result, virtually 
        unlimited distances between the primary and secondary disk subsystems are supported.
      </p>
      <p>
        Global Mirror requires FCP links on the disk subsystems. If the recovery site is within the distance 
        supported by FCP direct connect, switches, or DWDM, you can use one of those methods to connect the 
        primary and secondary disk subsystems. Otherwise, you must use network extension technology that supports FCP links.
      </p>
    <h5><b>Addressing z/OS device limits in a GDPS GM environment</b>
    </h5>
      <p>
        As clients implement IT resiliency solutions that rely on multiple copies of data, more are finding that 
        the z/OS limit of 64K (65,536) devices is limiting their ability to grow. Clients can consolidate data 
        sets to fewer larger volumes, but even with that, there are times when this might not make operational 
        sense for all types of data.   
      </p>
      <p>
        To this end, z/OS introduced the concept of an alternate subchannel set, which can include the definition for certain 
        types of disk devices. An alternate subchannel set provides another set of 64K devices for the following device types:
      </p>
      <ul>
        <li class="arrows">
          Parallel Access Volume (PAV) alias devices
        </li>
        <li class="arrows">
          Metro Mirror secondary devices (defined as 3390D)
        </li>
        <li class="arrows">
          FlashCopy target devices
        </li>
      </ul>
      
      <p>
        Including PAV alias devices in an alternate subchannel set is transparent to GDPS and is common practice for many 
        client configurations.
      </p>
      <p>
        The application site controlling system performs actions against the GM primary devices and can address up to nearly 
        64 K devices. The recovery site controlling system performs actions against the GM secondary and the GM FlashCopy devices. 
        GDPS supports defining the GM FlashCopy devices in an alternative subchannel set (MSS1) or not defining them at 
        all (which is known as <i>no-ucb FlashCopy</i>). This ability allows up to nearly 64 K devices to be 
        replicated in a GDPS GM environment.  
      </p>
    <h5><b>Summary</b>
    </h5>
      <p>
        Global Mirror provides an asynchronous remote copy offering that supports virtually unlimited distance, without the 
        requirement of an SDM system to move the data from primary to secondary volumes. Global Mirror also supports a wider 
        variety of platforms because it supports FB devices and removes the requirement for timestamped updates 
        that is imposed by XRC.   
      </p>
      <p>
        Conversely, Global Mirror is currently not as scalable as XRC because it supports only a maximum of 17 storage 
        subsystems. In addition, Global Mirror does not have the multiple vendor flexibility provided by XRC.  
      </p>
    <h4 id="Combining_disk_remote_copy_technologies_for_CA_and_DR"><b>Combining disk remote copy technologies for CA and DR</b></h4>
      <p>
        In this section we briefly describe Metro/Global Mirror and Metro/z/OS Global Mirror. For more detailed information, 
        see
        <a href="#Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery">Combining local and metro 
          continuous availability with out of region disaster recovery</a>. 
        Combining the technologies of Metro Mirror and HyperSwap with either Global Mirror or XRC (also referred to as <i>z/OS 
          Global Mirror</i> in this section) allows clients to meet requirements for continuous availability (CA) with zero data 
        loss locally within metropolitan distances for most failures, along with providing a disaster recovery (DR) solution 
        in the case of a region-wide disaster. This combination might also allow clients to meet increasing regulatory requirements. 
      </p>
    <h5><b>Metro Global Mirror</b>
    </h5>
      <p>
        Metro Global Mirror (MGM) is a cascading data replication solution that combines the capabilities of Metro Mirror 
        and Global Mirror.  
      </p>
      <p>
        Synchronous replication between a primary and secondary disk subsystem located either within a single data center, 
        or between two data centers located within metropolitan distances, is implemented using Metro Mirror. Global Mirror 
        is used to asynchronously replicate data from the secondary disks to a third disk subsystem in a recovery site 
        typically out of the local metropolitan region. As described in
        <a href="#Global_Mirror">Global Mirror</a>, a fourth set 
        of disks, also in the recovery site, are the FlashCopy targets used to provide the consistent data for disaster recovery.  
      </p>
      <p>
        MGM provides a comprehensive three-copy data replication strategy to protect against day-to-day disruptions, 
        while protecting critical business data and functions if there is a wide-scale disruption.
      </p>
    <h5><b>Metro z/OS Global Mirror</b>
        
    </h5>
          
      <p>
        GDPS Metro/z/OS Global Mirror (MzGM) is a multi-target data replication solution that combines the capabilities 
        of Metro Mirror and XRC (z/OS Global Mirror).      
      </p>
      <p>
        Synchronous replication between a primary and secondary disk subsystem located either within a single data center, 
        or between two data centers located within metropolitan distances, is implemented using Metro Mirror. XRC is used 
        to asynchronously replicate data from the primary disks to a third disk system in a recovery site, typically 
        out of the local metropolitan region. 
      </p>
      <p>
        Because XRC supports only CKD devices, only IBM Z data can be mirrored to the recovery site. However, because 
        Metro Mirror and XRC are supported by multiple storage vendors, this solution provides flexibility that MGM cannot.  
      </p>
      <p>
        For enterprises looking to protect IBM Z data, MzGM delivers a three-copy replication strategy to provide continuous 
        availability for day-to-day disruptions, while protecting critical business data and functions if there 
        is a wide-scale disruption. 
      </p>

    <h4 id="IBM_software_replication_products"><b>IBM software replication products</b></h4>
      <p>
        This section does not aim to provide a comprehensive list of all IBM software-based replication products. 
        Instead, it provides an introduction to the following supported products within the GDPS Continuous Availability solution: 
      </p>

      <ul>
        <li class="arrows">
          InfoSphere Data Replication for IMS for z/OS
        </li>
        <li class="arrows">
          InfoSphere Data Replication for VSAM for z/OS
        </li>
        <li class="arrows">
          InfoSphere Data Replication for DB2 for z/OS
        </li>
      </ul>
      <p>
        These products provide the capability to asynchronously copy changes to data held in IMS or DB2 databases or VSAM 
        files from a source to target copy. Fine-grained controls allow you to precisely define what data is critical to your 
        workload and needs to be copied in real time between the source and target. Unlike disk replication solutions 
        that are application, or data-agnostic and work at the z/OS volume level, software replication does not provide 
        a mechanism for copying all possible data types in your environment. As such, it is suited to provide only 
        a CA/DR solution for specific workloads that can tolerate only the IMS, DB2 or VSAM database-resident information 
        to be copied between locations. This is also discussed in 
        <a href="#GDPS_Continuous_Availability_solution">GDPS Continuous Availability solution</a>.
      </p>


      <h5><b>InfoSphere Data Replication for IMS for z/OS</b>
      </h5>
          
      <p>
        IMS Replication provides the mechanisms for producing copies of your IMS databases and maintaining the currency 
        of the data in near real time, typically between two systems separated by geographic distances. There is essentially 
        no limit to the distance between source and target systems because the copy technique is asynchronous and uses TCP/IP 
        as the protocol to transport the data over your wide area network (WAN).   
      </p>
      <p>
        IMS replication employs Classic data servers in the source and target systems to provide the replication services.  
      </p>
     
      <h5><b><em>Classic source server</em></b>
      </h5>
      <p>
        The Classic source server reads the IMS log data and packages changes to the specified databases into messages 
        that are then sent through TCP/IP to the target location.
      </p>
      <h5><b><em>Classic target server</em></b>
      </h5>
      <p>
        The Classic target server, running in the target location, receives messages from the source server and applies 
        the changes to a replica of the source IMS database in near real time. IMS replication provides mechanisms to 
        ensure that updates to a given record in the source database are applied in the same sequence in the target 
        replica. Furthermore, IMS replication maintains a <i>bookmark</i> to know where it has reached in processing the 
        IMS log data so that if any planned or unplanned outage occurs, it can later catch up knowing where it was 
        at the time of the outage.
      </p>
      <p>
        For more information,
        <a href="https://www.ibm.com/docs/en/SSEPH2_13.1.0/com.ibm.ims13.doc.rpg/ims_tlsenh_infosphere_imsrepl.htm">IBM Knowledge
          center.
        </a>
      </p>

      <h5><b>InfoSphere Data Replication for VSAM for z/OS</b>
      </h5>
        <p>
          VSAM replication is similar in structure to IMS replication. For CICS/VSAM workloads, the transaction data for 
          selected VSAM data sets is captured using the CICS log streams as the source. For non-CICS workloads, 
          CICS VSAM Recovery (CICS VR) logs are used as the source for capturing VSAM update information. The updates 
          are transmitted to the target using TCP/IP, where they are applied to the target data sets upon receipt.
        </p>
      <h5><b>InfoSphere Data Replication for DB2 for z/OS</b>
      </h5>
        <p>
          InfoSphere Replication Server for z/OS, as used in the GDPS Continuous Availability solutions, is also known as 
          <i>Q replication</i>. It provides a high capacity and low latency replication solution that uses IBM WebSphere® MQ message 
          queues to transmit data updates between source and target tables of a DB2 database.
        </p>
        <p>
          Q replication is split into two distinct pieces:
        </p>

          <ul>
            <li class="arrows">
              Q capture program or engine
            </li>
            <li class="arrows">
              Q apply program or engine
            </li>
          </ul>

      <h5><b><em>Q capture</em></b>
      </h5>
        <p>
          The Q capture program reads the DB2 logs or changes to the source table or tables that you 
          want to replicate. These changes are then put into WebSphere MQ messages and sent across the 
          WebSphere MQ infrastructure to the system where the target table resides. There, they are 
          read and applied to the target table by the Q apply program.
        </p>
        <p>
          The Q capture program is flexible in terms of what can be included or excluded from the data 
          sent to the target and even the rate at which data is sent can be modified if required.
        </p>
        <p>
          By the nature of the method of Q replication, the replication of data is an asynchronous process. 
          Even so, an RPO of a few seconds is possible even in high update environments.
        </p>
      <h5><b><em>Q apply</em></b>
      </h5>
        <p>
          The Q apply program takes WebSphere MQ messages from a receive queue, or queues and then applies the 
          changes held within the message to the target tables. The Q apply program is designed in such a way 
          to use parallelism to keep up with updates to multiple targets while maintaining any referential 
          integrity constraints between related target tables.
        </p>
        <p>
          Both the Q capture and Q apply programs have mechanisms to track what has been read from the logs and 
          sent to the target site, and what has been read from the receive queues and applied to the target tables, 
          including any dependencies between updates.
        </p>
        <p>
          This in turn provides data consistency and allows for restart of both the capture and apply programs, 
          if this is required or in case of failures.
        </p>
        <p>
          For more information about Q replication, see
          <a href="https://www.ibm.com/docs/en/db2/9.7?topic=SSEPGG_9.7.0/com.ibm.swg.im.iis.prod.repl.nav.doc/topics/iiyrqcncreplepovu.html">
            IBM Knowledge center.</a>
        </p>

    <h3 id="Tape_resident_data"><b>Tape resident data</b></h3>
      <p><i>Operational data</i>, that is, data that is used directly by applications supporting users, is normally found on disk. 
        However, there is another category of data (called <i>support data</i>) that supports the operational data; this often 
        resides in tape subsystems. Support data typically covers migrated data, point-in-time backups, archive data, 
        and so on. For sustained operation in the failover site, the support data is indispensable. Furthermore, some 
        enterprises have mission-critical data that resides only on tape. You need a solution to ensure that tape data 
        is readily accessible at your recovery site. 
      </p>
      <p>
        Just as you mirror your disk-resident data to protect it, similarly you can mirror your tape-resident data. GDPS 
        provides support for management of the IBM Virtualization Engine TS7700. <em>(The TS7700 management support is 
        available only in GDPS Metro at this time.)</em> 
        See section
        <a href="#Protecting_tape_data">Protecting tape data</a> for details about GDPS TS7700 support. 
        The IBM Virtualization Engine TS7700 provides comprehensive support for replication of tape data. 
        For more information about the TS7700 technology that complements GDPS for tape data, see <i>IBM Virtualization 
          Engine TS7700 with R 2.0</i>, SG24-7975.
      </p>
    <h3 id="FlashCopy"><b>FlashCopy</b></h3>
      <p>
        FlashCopy provides a point-in-time (PiT) copy of a volume, with almost instant availability for the user of both the 
        source and target volumes. There is also a data set-level FlashCopy supported for z/OS volumes. Only a minimal 
        interruption is required for the FlashCopy relationship to be established. The copy is then created by the disk 
        subsystem, with minimal impact on other disk subsystem activities. The volumes created when you use FlashCopy to 
        copy your secondary volumes are called <i>tertiary volumes</i>.
      </p>
    <h5><b><em>FlashCopy and disaster recovery</em></b>
    </h5>
      <p>
        FlashCopy has specific benefits in relation to disaster recovery. For example, consider what happens if you temporarily 
        lose connectivity between primary and secondary Metro Mirror volumes. At the point of failure, the secondary volumes 
        will be consistent. However, during the period when you are resynchronizing the primary and secondary volumes, the secondary 
        volumes are inconsistent (because the updates are not applied in the same time sequence that they were written to the primaries). 
        So, what happens if you have a disaster during this period? If it is a real disaster, your primary disk subsystem will be 
        a smoldering lump of metal on the computer room floor. And your secondary volumes are inconsistent, so those volumes are 
        of no use to you either.
      </p>
      <p>
        So, how do you protect yourself from such a scenario? One way (our suggested way) is to take a FlashCopy of the 
        secondary volumes just before you start the resynchronization process. This at least ensures that you have a 
        consistent set of volumes in the recovery site. The data might be several hours behind the primary volumes, 
        but even data a few hours old that is consistent is better than current, but unusable, data.
      </p>
      <p>
        An additional benefit of FlashCopy is that it provides the ability to perform disaster recovery tests while still 
        retaining disaster recovery readiness. The FlashCopy volumes you created when doing the resynchronization (or subsequently) 
        can be used to enable frequent testing (thereby ensuring that your recovery procedures continue to be effective) without 
        having to use the secondary volumes for that testing.
      </p>
      <p>
        FlashCopy can operate in several modes. GDPS uses one of the following modes of FlashCopy, depending on the GDPS offering:
      </p>

<!-- Tablelike GRID FOR LAYOUT -->

<!-- GRIDS - one for larger screens and other for smaller - toggle display 'none' in styles.css -->

<!-- GRID - for larger screens  -->
      
<div class="flashcopy-grid-large-screen">
  <div class="grid-item-flashcopy1"><b>COPY</b></div>
  <div class="grid-item-flashcopy2">When the volumes are logically copied, the FlashCopy session continues as a 
    background operation, physically copying all the data from the source volume to the target. 
    When the volumes have been physically copied, the FlashCopy session ends. In this mode, 
    the FlashCopy target physical volume will be a mirror image of the source volume at the 
    time of the FlashCopy.
  </div>
  <div class="grid-item-flashcopy3"><b>NOCOPY</b></div>
  <div class="grid-item-flashcopy4">When the volumes are logically copied, a FlashCopy session continues as 
    a background operation, physically copying only those tracks subsequently updated by write operations 
    to the source volume. In this mode, the FlashCopy target physical volume contains only data that 
    was changed on the source volume after the FlashCopy.
  </div>
  <div class="grid-item-flashcopy5"><b>NOCOPY2COPY</b></div>
  <div class="grid-item-flashcopy6">Change existing FlashCopy relationship from NOCOPY to COPY. 
    This can be done dynamically. When one or more NOCOPY relationships exist for a source volume, 
    NOCOPY2COPY will initiate a background copy for all target relationships with intersecting 
    source extents from the point in time the NOCOPY was issued. Upon completion of the background 
    copy, the converted relationship or relationships will be terminated.
  </div>
  <div class="grid-item-flashcopy7"><b>INCREMENTAL</b>
  </div>
  <div class="grid-item-flashcopy8">This allows repetitive FlashCopies to be taken, but only the 
    tracks that have changed since the last FlashCopy will be copied to the target volume. This provides 
    the ability to refresh a FlashCopy relationship and bring the target up to the source’s newly established 
    point-in-time. Incremental FlashCopy helps reduce the background copy completion time when only a subset 
    of data on either the source or target has changed, thus giving you the option to perform a FlashCopy 
    on a more frequent basis.
  </div>
  <div class="grid-item-flashcopy9"><b>CONSISTENT</b>
  </div>
  <div class="grid-item-flashcopy10">This option is applicable to GDPS Metro and GDPS Metro HyperSwap Manager 
    environments. It creates a consistent set of tertiary disks without suspending Metro Mirror. 
    It uses the FlashCopy Freeze capability which, similar to Metro Mirror Freeze, puts all source disks 
    in Extended Long Busy to ensure that the FlashCopy source disks are consistent before the point-in-time 
    copy is made. After the source disks are consistent, the FlashCopy is taken (quite fast) and the Freeze is thawed.
    <br>
    <br>
    Without this support, you would need to suspend Metro Mirror (planned freeze) and then resynchronize Metro Mirror 
    to produce a consistent point-in-time copy of the secondary disks. HyperSwap would remain disabled from 
    the time you suspended Metro Mirror until the mirror is full-duplex again; however, this can take a long 
    time depending on how much data was updated while Metro Mirror remained suspended. In comparison, with 
    Consistent FlashCopy, HyperSwap is only disabled during the FlashCopy Freeze, which should be simply a few seconds.
<br>
<br>

    GDPS gives you the capability to restrict the FlashCopy Freeze duration and to abort the FlashCopy operation if the 
    FlashCopy Freeze time exceeds your threshold.
    <br>
    <br>
    To create a consistent point-in-time copy of the primary disks without Consistent FlashCopy, you would need to somehow 
    make sure that there is no I/O on the primary disks (effectively, you would need to stop the production systems). 
    With Consistent FlashCopy, production systems continue to run and I/O is prevented during the few seconds 
    until the FlashCopy Freeze completes. After the FlashCopy Freeze completes, the primary disks are in a consistent state, 
    the FlashCopy operation itself is quite fast, and then the freeze is thawed and production systems resume I/O. 
    Consistent FlashCopy can be used in conjunction with COPY, NOCOPY, or INCREMENTAL FlashCopy.
  </div>
  <div class="grid-item-flashcopy11"><b>Zero Suspend</b>
  </div>
  <div class="grid-item-flashcopy12">This option is applicable to GDPS XRC environments. It creates a recoverable set of 
    tertiary disks for recovery testing with no suspension of the XRC operation. This allows DR testing to be 
    performed without ever losing the DR capability. Before this support, to produce a consistent tertiary 
    copy you needed to suspend XRC for all volumes, FlashCopy secondary volumes, and then resynchronize XRC sessions.
  </div>

</div>

<!-- GRID - for smaller screens  -->

<div class="flashcopy-grid-small-screen">
  <div class="grid-item-flashcopy1"><b>COPY</b></div>
  <div class="grid-item-flashcopy2">When the volumes are logically copied, the FlashCopy session continues as a 
    background operation, physically copying all the data from the source volume to the target. 
    When the volumes have been physically copied, the FlashCopy session ends. In this mode, 
    the FlashCopy target physical volume will be a mirror image of the source volume at the 
    time of the FlashCopy.
  </div>
  <div class="grid-item-flashcopy3"><b>NOCOPY</b></div>
  <div class="grid-item-flashcopy4">When the volumes are logically copied, a FlashCopy session continues as 
    a background operation, physically copying only those tracks subsequently updated by write operations 
    to the source volume. In this mode, the FlashCopy target physical volume contains only data that 
    was changed on the source volume after the FlashCopy.
  </div>
  <div class="grid-item-flashcopy5"><b>NOCOPY2COPY</b></div>
  <div class="grid-item-flashcopy6">Change existing FlashCopy relationship from NOCOPY to COPY. 
    This can be done dynamically. When one or more NOCOPY relationships exist for a source volume, 
    NOCOPY2COPY will initiate a background copy for all target relationships with intersecting 
    source extents from the point in time the NOCOPY was issued. Upon completion of the background 
    copy, the converted relationship or relationships will be terminated.
  </div>
  <div class="grid-item-flashcopy7"><b>INCREMENTAL</b>
  </div>
  <div class="grid-item-flashcopy8">This allows repetitive FlashCopies to be taken, but only the 
    tracks that have changed since the last FlashCopy will be copied to the target volume. This provides 
    the ability to refresh a FlashCopy relationship and bring the target up to the source’s newly established 
    point-in-time. Incremental FlashCopy helps reduce the background copy completion time when only a subset 
    of data on either the source or target has changed, thus giving you the option to perform a FlashCopy 
    on a more frequent basis.
  </div>
  <div class="grid-item-flashcopy9"><b>CONSISTENT</b>
  </div>
  <div class="grid-item-flashcopy10">This option is applicable to GDPS Metro and GDPS Metro HyperSwap Manager 
    environments. It creates a consistent set of tertiary disks without suspending Metro Mirror. 
    It uses the FlashCopy Freeze capability which, similar to Metro Mirror Freeze, puts all source disks 
    in Extended Long Busy to ensure that the FlashCopy source disks are consistent before the point-in-time 
    copy is made. After the source disks are consistent, the FlashCopy is taken (quite fast) and the Freeze is thawed.
    Without this support, you would need to suspend Metro Mirror (planned freeze) and then resynchronize Metro Mirror 
    to produce a consistent point-in-time copy of the secondary disks. HyperSwap would remain disabled from 
    the time you suspended Metro Mirror until the mirror is full-duplex again; however, this can take a long 
    time depending on how much data was updated while Metro Mirror remained suspended. In comparison, with 
    Consistent FlashCopy, HyperSwap is only disabled during the FlashCopy Freeze, which should be simply a few seconds.
    GDPS gives you the capability to restrict the FlashCopy Freeze duration and to abort the FlashCopy operation if the 
    FlashCopy Freeze time exceeds your threshold.
    To create a consistent point-in-time copy of the primary disks without Consistent FlashCopy, you would need to somehow 
    make sure that there is no I/O on the primary disks (effectively, you would need to stop the production systems). 
    With Consistent FlashCopy, production systems continue to run and I/O is prevented during the few seconds 
    until the FlashCopy Freeze completes. After the FlashCopy Freeze completes, the primary disks are in a consistent state, 
    the FlashCopy operation itself is quite fast, and then the freeze is thawed and production systems resume I/O. 
    Consistent FlashCopy can be used in conjunction with COPY, NOCOPY, or INCREMENTAL FlashCopy.
  </div>
  <div class="grid-item-flashcopy11"><b>Zero Suspend</b>
  </div>
  <div class="grid-item-flashcopy12">This option is applicable to GDPS XRC environments. It creates a recoverable set of 
    tertiary disks for recovery testing with no suspension of the XRC operation. This allows DR testing to be 
    performed without ever losing the DR capability. Before this support, to produce a consistent tertiary 
    copy you needed to suspend XRC for all volumes, FlashCopy secondary volumes, and then resynchronize XRC sessions.
  </div>

</div>

<!-- END OF 'flashcopy' GRID  -->

  <p>
    If you plan to use FlashCopy, remember that the source and target volumes must be within the same physical 
    disk subsystem. This is a capacity planning consideration when configuring and planning for the growth of 
    your disk subsystems.
  </p>
  <p>
    Also remember that if you performed a site switch to run in the recovery site, at some point you will want to 
    return to the production site. To provide equivalent protection and testing capability no matter which site 
    you are running in, consider providing FlashCopy capacity in both sites.
  </p>
  <p>
    Furthermore, GDPS does not perform FlashCopy for simply selected volumes. The GDPS use of FlashCopy is for the purposes 
    of protection during resynchronization and for testing. Both of these tasks require that a point-in-time copy for the 
    entire configuration is made. GDPS FlashCopy support assumes that you will provide FlashCopy target devices for the 
    entire configuration and that every time GDPS performs a FlashCopy, it will be for all secondary devices (GDPS Metro 
    also supports FlashCopy for primary devices).
  </p>
  <p>
    An exception to this rule is that GDPS can perform FlashCopy for a subset of the production volumes when FlashCopy is 
    used for the purposes Logical Corruption Protection (LCP). For more information about how GDPS uses FlashCopy technology 
    to provide flexible testing and protection against various types of logical data corruption, including cyber attacks and 
    internal threats, see 
    <a href="#Introduction_to_LCP_and_Testcopy_Manager">Introduction to LCP and Testcopy Manager</a>.
  </p>

  <h5><b>User-initiated FlashCopy</b>
  </h5>
  <p>
    User-initiated FlashCopy supports FlashCopy of all defined FlashCopy volumes using panel commands, GDPS scripts, 
    or GDPS Z NetView for z/OS commands, depending on which GDPS product is used.
  </p>

  <h5><b>Space-efficient FlashCopy (FlashCopy SE)</b>
  </h5>
  <p>
    FlashCopy SE is functionally not much different from the standard FlashCopy. The concept of <i>space-efficient</i> with 
    FlashCopy SE relates to the attributes or properties of a DS8000 volume. As such, a space-efficient volume can be 
    used like any other DS8000 volume.
  </p>
  <p>
    When a normal volume is created, it occupies the defined capacity on the physical drives. A space-efficient volume 
    does not occupy physical capacity when it is initially created. Space gets allocated when data is actually written 
    to the volume. This allows the FlashCopy target volume capacity to be thinly provisioned (that is, smaller than the 
    full capacity of the source volume). In essence this means that when planning for FlashCopy, you may provision less 
    disk capacity when using FlashCopy SE than when using standard FlashCopy, which can help lower the amount of physical 
    storage needed by many installations
  </p>
  <p>
    All GDPS products support FlashCopy SE. Details of how FlashCopy SE is used by each offering is described in the 
    chapter related to that offering.
  </p>

  <h3 id="Automation"><b>Automation</b></h3>
  <p>
    If you have challenging recovery time and recovery point objectives, implementing disk remote copy, software-based 
    replication, tape remote copy, FlashCopy, and so on are necessary prerequisites for you to be able to recover from a 
    disaster and meet your objectives. However, be sure you realize that they are only enabling technologies. To achieve 
    the stringent objectives placed on many IT departments today, it is necessary to tie those technologies together with 
    automation and sound systems management practices. In this section we discuss your need for automation to recover from an outage.
  </p>
  <h4 id="Recovery_time_objective"><b>Recovery time objective</b></h4>
  <p>
    If you have reached this far in the document, we presume that your recovery time objective (RTO) is a “challenge” to you. 
    If you have performed tape-based disaster recovery tests, you know that ensuring that all your data is backed up is only 
    the start of your concerns. In fact, even getting all those tapes restored does not result in a mirror image of your 
    production environment. You also need to get all your databases up to date, get all systems up and running, and then 
    finally start all your applications.
  </p>
  <p>
    Trying to drive all this manually will, without question, prolong the whole process. Operators must react to events as 
    they happen, while consulting recovery documentation. However, automation responds at machine speeds, meaning your 
    recovery procedures will be executed without delay, resulting in a shorter recovery time.
  </p>
<h4 id="Operational_consistency"><b>Operational consistency</b></h4>
  <p>
    Think about an average computer room scene immediately following a system failure. All the phones are ringing. Every 
    manager within reach moves in to determine when everything will be recovered. The operators are frantically scrambling for 
    procedures that are more than likely outdated. And the systems programmers are all vying with the operators for control of 
    the consoles; in short, chaos.
  </p>
  <p>
    Imagine, instead, a scenario where the only manual intervention is to confirm how to proceed. From that point on, 
    the system will recover itself using well-tested procedures. How many people watch it does not matter because it will not
    make mistakes. And you can yell at it all you like, but it will still behave in exactly the manner it was in which it was 
    programmed to behave. You do not need to worry about outdated procedures being used. The operators can concentrate on 
    handing calls and queries from the assembled managers. And the systems programmers can concentrate on pinpointing the 
    cause of the outage, rather than trying to get everything up and running again.
  </p>
  <p>
    And all of this is just for a system outage. Can you imagine the difference that well-designed, coded, and tested automation 
    can make in recovering from a real disaster? Apart from speed, perhaps the biggest benefit that automation brings is consistency. 
    If your automation is thoroughly tested, you can be assured that it will behave in the same way, time after time. When recovering 
    from as rare an event as a real disaster, this consistency can be a lifesaver.
  </p>
  <h4 id="Skills_impact"><b>Skills impact</b></h4>
  <p>
    Recovering a computing center involves many complex activities. Training staff takes time. People come and go. You 
    cannot be assured that the staff that took part in the last disaster recovery test will be on hand to drive recovery 
    from this real disaster. In fact, depending on the nature of the disaster, your skilled staff might not even be available 
    to drive the recovery.  
  </p>
  <p>
    The use of automation removes these concerns as potential pitfalls to your successful recovery.
  </p>
  <h4 id="Summary"><b>Summary</b></h4>
  <p>
    The technologies you will use to recover your systems all have various control interfaces. Automation is required to tie them 
    all together so they can be controlled from a single point and your recovery processes can be executed quickly and consistently.
  </p>
  <p>
    Automation is one of the central tenets of the GDPS offerings. By using the automation provided by GDPS, you save all the 
    effort to design and develop this code yourself, and also benefit from the IBM experience with hundreds of clients 
    across your industry and other industries.
  </p>

  <h3 id="Flexible_server_capacity"><b>Flexible server capacity</b></h3>
  <p>
    In this section we discuss options for increasing your server capacity concurrently, for either planned upgrades or unplanned upgrades, 
    to quickly provide the additional capacity you will require on a temporary basis. These capabilities can be used for server or site 
    failures, or they can be used to help meet the temporary peak workload requirements of clients.
  </p>
  <p>
    The only capabilities described in this section are the ones used by GDPS. Other capabilities exist to upgrade server capacity, 
    either on a temporary or permanent basis, but they are not covered in this section.
  </p>
  <h4 id="Capacity_Backup_upgrade"><b>Capacity Backup upgrade</b></h4>
  <p>
    Capacity Backup (CBU) upgrade for IBM Z processors provides reserved emergency backup server capacity that can be activated 
    in lieu of capacity that is lost as a result of an unplanned event elsewhere. CBU helps you to recover by adding reserved 
    capacity on a designated IBM Z system. A CBU system normally operates with a base server configuration and with 
    a preconfigured number of additional processors reserved for activation in case of an emergency.
  </p>
  <p>
    CBU can be used to install (and pay for) less capacity in the recovery site than you have in your production site, 
    while retaining the ability to quickly provision the additional capacity that would be required in a real disaster.
  </p>
  <p>
    CBU can be activated manually, using the HMC. It can also be activated automatically by GDPS, either as part of 
    a disaster recovery test, or in reaction to a real disaster. Activating the additional processors is nondisruptive. 
    That is, you do not need to power-on reset (POR) the server or even IPL the LPARs that can benefit from the 
    additional capacity (assuming that an appropriate number of reserved CPs were defined in the LPAR Image profiles).
  </p>
  <p>
    CBU is available for all processor types on IBM Z.
  </p>
  <p>
    The CBU contract allows for an agreed-upon number of tests over the period of the contract. GDPS supports 
    activating CBU for test purposes.
  </p>
  <p>
    For more information about CBU, see <i>System z Capacity on Demand User’s Guide</i>, SC28-6846.
  </p>
  <h4 id="On/Off_Capacity_on_Demand"><b>On/Off Capacity on Demand</b></h4>
  <p>
    On/Off Capacity on Demand (On/Off CoD) is a function that enables concurrent and temporary capacity growth of the server. 
    The difference between CBU and On/Off CoD is that On/Off CoD is for planned capacity increases, 
    and CBU is intended to replace capacity lost as a result of an unplanned event elsewhere. 
    On/Off CoD can be used for client peak workload requirements, for any length of time, and it has a 
    daily hardware and software charge.
  </p>
  <p>
    On/Off CoD helps clients, with business conditions that do not justify a permanent upgrade in capacity, to 
    contain workload spikes that might exceed permanent capacity so that Service Level Agreements cannot be met. 
    On/Off CoD can concurrently add processors (CPs, IFLs, ICFs, zAAPs, and zIIPs) up to the limit of the 
    installed books of an existing server. It is restricted to double the currently installed capacity.
  </p>
  <h4 id="Capacity_for_Planned_Events"><b>Capacity for Planned Events</b></h4>
  <p>
    Capacity for Planned Events (CPE) can be used to replace capacity because of relocation of workloads, such 
    as during system migrations, data center or server relocation, re-cabling, or general work on the physical 
    infrastructure of the data processing environment.
  </p>
  <p>
    CPE provides the ability to concurrently and temporarily (for 72 hours) activate more CPs, ICFs, IFLs, zAAPs, 
    zIIPs, and SAPs to increase the CP capacity level, or a combination of these.
  </p>
  <h4 id="System_Recovery_Boost"><b>System Recovery Boost</b></h4>
  <p>
    System Recovery Boost (SRB) delivers substantially faster system shutdown and restart, short duration recovery 
    process boosts for sysplex events (such as HyperSwap events), and enables faster catch up of the accumulated 
    backlog of work after specific events, such as system restart.
  </p>
  <p>
    SRB is available starting with the IBM z15™ IBM Z processor.
  </p>
  <h4 id="GDPS_CBU,_On/Off_CoD,_CPE,_and_SRB"><b>GDPS CBU, On/Off CoD, CPE, and SRB</b></h4>
  <p>
    The GDPS temporary capacity management capabilities are related to the capabilities provided by the particular 
    server system being provisioned. Processors before the
    IBM Z10 required that the full capacity for a Capacity Backup (CBU) upgrade or On/Off Capacity on Demand 
    (OOCoD) be activated, even though the full capacity might not be required for the particular situation at hand.
  </p>
  <p>
    GDPS, with IBM Z10 and later generation systems, provides support for activating temporary capacity, 
    such as CBU and OOCoD, based on a preinstalled capacity-on-demand record. In addition to the capability to 
    activate the full record, GDPS also provides the ability to define profiles that determine what will be activated. 
    The profiles are used in conjunction with a GDPS script statement and provide the flexibility to activate 
    the full record or a partial record.
  </p>
  <p>
    When temporary capacity upgrades are performed by using GDPS facilities, GDPS tracks activated CBU and OOCoD 
    resources at a Central Electronics Complex (CEC) level.
  </p>
  <p>
    GDPS provides keywords in GDPS scripts to support activation and deactivation of the CBU, On/Off CoD, CPE, and SRB functions.
  </p>
  <p>
    GDPS allows definition of capacity profiles to add capacity to already running systems. Applicable types of reserved engines 
    (CPs, zIIPs, zAAPs, IFLs, and ICFs) can be configured online to GDPS z/OS systems, to xDR-managed z/VM systems, and to 
    coupling facilities that are managed by GDPS.
  </p>
  <p>
    When a GDPS z/OS system is IPLed, GDPS automatically configures online any applicable reserved engines 
    (CPs, zIIPs, and zAAPs) based on the LPAR profile. The online configuring of reserved engines is done only if 
    temporary capacity was added to the CEC where the system is IPLed using GDPS facilities.
  </p>
<h3 id="Cross_-_site_connectivity_considerations"><b>Cross-site connectivity considerations</b>
</h3>
  <p>
    When setting up a recovery site, there might be a sizeable capital investment to get started, but you might find 
    that one of the largest components of your ongoing costs is related to providing connectivity between the sites. 
    Also, the type of connectivity available to you can affect the recovery capability you can provide. Conversely, 
    the type of recovery capability you want to provide will affect the types of connectivity you can use.
  </p>
  <p>
    In this section, we list the connections that must be provided, from a simple disk remote copy configuration through to 
    an Active/Active workload configuration. We briefly review the types of cross-site connections that you must 
    provide for the different GDPS solutions and the technology that must be used to provide that connectivity. 
    All of these descriptions relate solely to cross-site connectivity. We assume that you already have whatever 
    intrasite connectivity is required.
  </p>
<h4 id="Server_-_to-_disk_links"><b>Server-to-disk links</b></h4>
  <p>
    If you want to be able to use disks installed remotely from a system in the production site, you must provide channel 
    connections to those disk control units.
  </p>

  <h5><b>Metro Mirror and MTMM-based solutions</b>
  </h5>
  <p>
    For Metro Mirror and MTMM with GDPS, all of the secondary disks (both sets for MTMM) must be defined to and 
    channel-accessible to the production systems for GDPS to be able to manage those devices.
  </p>
  <p>
    If you foresee a situation where systems in the production site will be running off the secondary disks (for example, 
    if you will use HyperSwap), you need to provide connectivity equivalent to that provided to the corresponding primary 
    volumes in the production site. The HyperSwap function provides the ability to nondisruptively swap from the primary 
    volume of a mirrored pair to what had been the secondary volume.
  </p>
  <p>
    If you do not have any cross-site disk accessing, minimal channel bandwidth (two FICON channel paths from each system 
    to each disk subsystem) is sufficient.
  </p>
  <p>
    Depending on your director and switch configuration, you might be able to share the director-to-director links between 
    channel and Metro Mirror connections. For more information, see <i>IBM System z Connectivity Handbook</i>, SG24-5444.
  </p>
  <h5 id="Server_-_to-_disk_links"><b><em>HyperSwap across sites with less than full channel bandwidth</em></b></h5>
  <p>
    You might consider enabling unplanned HyperSwap to the secondary disks in the remote site even if you do not have sufficient 
    cross-site channel bandwidth to sustain your production workload for normal operations. Assuming that a disk failure is 
    likely to cause an outage and you will need to switch to using a disk in the other site, the unplanned HyperSwap might 
    at least give you the opportunity to perform an orderly shutdown of your systems first. Shutting down your systems 
    cleanly avoids the complications and longer restart time that is associated with crash-restart of application subsystems.
  </p>

  <p>
    For GDPS Metro environments, the same consideration applies to enabling HyperSwap to the remote secondary copy: Channel 
    bandwidth to the local secondary copy should not be an issue.
  </p>

  <h5 id="XRC_-_based_and_Global_Mirror_-_based_solutions"><b>XRC-based and Global Mirror-based solutions</b></h5>
  <p>
    For any of the asynchronous remote copy implementations (XRC or Global Mirror), the production systems would normally 
    not have channel access to the secondary volumes.
  </p>


  <h5 id="Software_replication_-_based_solutions"><b>Software replication-based solutions</b></h5>
  <p>
    As with other asynchronous replication technologies, given that effectively unlimited distances are supported, there is 
    no requirement for the source systems to have host channel connectivity to the data in the target site.
  </p>

  <h4 id="Data_replication_links"><b>Data replication links</b></h4>
  <p>
    You need connectivity for your data replication activity for the following circumstances:
  </p>
  <ul>
    <li class="arrows">
      Between storage subsystems (for Metro Mirror or Global Mirror)
    </li>
    <li class="arrows">
      From the SDM system to the primary disks (for XRC)
    </li>
    <li class="arrows">
      Across the wide area network for software-based replication
    </li>
  </ul>

  <h5 id="Metro_Mirror_-_based_and_Global_Mirror_-_based_solutions"><b>Metro Mirror-based and Global Mirror-based solutions</b></h5>
  <p>
    The IBM Metro Mirror (including MTMM) and Global Mirror implementations use Fibre Channel Protocol (FCP) links between the primary 
    and secondary disk subsystems. The FCP connection can be direct, through a switch, or through other supported distance solutions 
    (for example, Dense Wave Division Multiplexer, DWDM, or channel extenders).
  </p>
  <h5 id="XRC_-_based_solutions"><b>XRC-based solutions</b></h5>
  <p>
    If you are using XRC, the System Data Movers (SDMs) are typically in the recovery site. The SDMs must have connectivity to both the 
    primary volumes and the secondary volumes. The cross-site connectivity to the primary volumes is a FICON connection, and depending 
    on the distance between sites, either a supported DWDM can be used (distances less than 300 km) or a channel extender can be used 
    for longer distances. As discussed in
    <a href="#Extended_Distance_FICON">Extended Distance FICON</a>, an enhancement to the industry standard FICON architecture 
    (FC-SB-3) helps avoid degradation of performance at extended distances, and this might also benefit XRC applications within 300 km 
    where channel extension technology had previously been required to obtain adequate performance.
  </p>
  <h5 id="Software_-_based_solutions"><b>Software-based solutions</b></h5>
  <p>
    Both IMS replication and DB2 replication use your wide area network (WAN) connectivity between the data source 
    and the data target. Typically, for both, either natively or through IBM WebSphere MQ, TCP/IP is the transport 
    protocol used, although other protocols, such as LU6.2, are supported. It is beyond the scope of this book to 
    go into detail about WAN design, but ensure that any such connectivity between the source and target have 
    redundant routes through the network to ensure resilience from failures. There are effectively no distance 
    limitations on the separation between source and target. However, 
    the greater the distance between them will affect the latency and, therefore, the RPO that can be achieved,
  </p>
  <h4 id="Coupling_links"><b>Coupling links</b></h4>
  <p>
    Coupling links are required in a Parallel Sysplex configuration to provide connectivity from the z/OS images to the 
    coupling facility. Coupling links are also used to transmit timekeeping messages when Server Time Protocol (STP) 
    is enabled. If you have a multisite Parallel Sysplex, you will need to provide coupling link connectivity between sites.  
  </p>
  <p>
    For distances greater than 10 km, either ISC3 or Parallel Sysplex InfiniBand (PSIFB) Long Reach links must be 
    used to provide this connectivity. The maximum supported distance depends on several things, including 
    the particular DWDMs that are being used and the quality of the links.
  </p>
  <p>
    <a href="#Table_2-1" class="links-within-text">Table 2-1</a> lists the distances that are supported by the various link types.
  </p>





 

<!-- describing the table -->
  <p id="Table_2-1">
    <i>Table 2-1 Supported CF link distances</i>
  </p>

<!-- Table with GRID FOR LAYOUT -->

<!-- GRIDS - one for larger screens and other for smaller - toggle display 'none' in styles.css -->

<!-- GRID - for larger screens  -->
  
    <div class="distances-link-types-grid-large">
      <div class="grid-item-distances-1"><b>Link type</b></div>
      <div class="grid-item-distances-2"><b>Link data rate</b></div>
      <div class="grid-item-distances-3"><b>Maximum unrepeated distance</b></div>
      <div class="grid-item-distances-4"><b>Maximum repeated distance</b></div>
      <div class="grid-item-distances-5">ISC-3</div>
      <div class="grid-item-distances-6">2 Gbpsa
        1 Gbpsb</div>
      <div class="grid-item-distances-7">10 km
        20 kmc</div>
      <div class="grid-item-distances-8">200 km</div>
      <div class="grid-item-distances-9">PSIFB Long Reach 1X</div>
      <div class="grid-item-distances-10">5.0 Gbps
        2.5 Gbpsd</div>
      <div class="grid-item-distances-11">10 km</div>
      <div class="grid-item-distances-12">175 km</div>
      <div class="grid-item-distances-13">PSIFB 12X, for use within a data center</div>
      <div class="grid-item-distances-14">6 GBytes/sec
        3 GBytes/sece</div>
      <div class="grid-item-distances-15">150 meters</div>
      <div class="grid-item-distances-16">Not applicable</div>

    </div>

<!-- GRID - for smaller screens  -->

<div class="distances-link-types-grid-small">
  <div class="grid-item-distances-1"><b>Link type</b></div>
  <div class="grid-item-distances-5">ISC-3</div>
  <div class="grid-item-distances-9">PSIFB Long Reach 1X</div>
  <div class="grid-item-distances-13">PSIFB 12X, for use within a data center</div>

  <div class="grid-item-distances-2"><b>Link data rate</b></div>
  <div class="grid-item-distances-6">2 Gbpsa
    1 Gbpsb</div>
  <div class="grid-item-distances-10">5.0 Gbps
      2.5 Gbpsd</div>
  <div class="grid-item-distances-14">6 GBytes/sec
        3 GBytes/sece</div>

  <div class="grid-item-distances-3"><b>Maximum unrepeated distance</b></div>
  <div class="grid-item-distances-7">10 km
    20 kmc</div>
    <div class="grid-item-distances-11">10 km</div>
    <div class="grid-item-distances-15">150 meters</div>

  <div class="grid-item-distances-4"><b>Maximum repeated distance</b></div>
  <div class="grid-item-distances-8">200 km</div>
  <div class="grid-item-distances-12">175 km</div>
  <div class="grid-item-distances-16">Not applicable</div>

</div>

<!-- END OF GRID -->

<!-- format the below into a, b, c list later -->
<p>a. Gbps (gigabits per second).</p>
<p>b. RPQ 8P2197 provides an ISC-3 Daughter Card that clocks at 1 Gbps.</p>
<p>c. Requires RPQ 8P2197 and 8P2263 (IBM Z Extended Distance).</p>
<p>d. The PSIFB Long Reach feature will negotiate to 1x IB-SDR link data rate 
  of 2.5 Gbps if connected to qualified DWDM infrastructure that cannot support the 5 Gbps (1x IB-DDR) rate.</p>
<p>e. The PSIFB links negotiate to 12x IB-SDR link data rate of 3 GBytes/sec when connected to IBM Z9 servers.</p>

<h4 id="Server_Time_Protocol"><b>Server Time Protocol</b></h4>
<p>
  Server Time Protocol (STP) is a server-wide facility that is implemented in the Licensed Internal 
  Code (LIC) of the IBM Z servers. It provides the capability for multiple servers to maintain time synchronization 
  with each other. STP is the successor to the 9037 Sysplex Timer. 
</p>
<p>
  STP is designed for servers that have been configured to be in a Parallel Sysplex or a basic sysplex 
  (without a coupling facility), and servers that are not in a sysplex, but need to be time-synchronized. 
  STP is a message-based protocol in which timekeeping information is passed over data links between servers. 
  The timekeeping information is transmitted over externally defined coupling links. Coupling links are used to 
  transport STP messages. 
</p>
<p>
  If you are configuring a sysplex across two or more sites, you need to synchronize servers in multiple sites. 
  For more information about Server Time Protocol, see <i>Server Time Protocol Planning Guide</i>, SG24-7280, and 
  <i>Server Time Protocol Implementation Guide</i>, SG24-7281.
</p>
<h4 id="XCF_signaling"><b>XCF signaling</b></h4>
<p>
  One of the requirements for being a member of a sysplex is the ability to maintain XCF communications 
  with the other members of the sysplex. XCF uses two mechanisms to communicate between systems: XCF signaling 
  structures in a CF and channel-to-channel adapters. Therefore, if you are going to have systems in both 
  sites that are members of the same sysplex, you must provide CF connectivity, CTC connectivity, or 
  preferably both, between the sites.
</p>
<p>
  If you provide both CF structures and CTCs for XCF use, XCF will dynamically determine which of the 
  available paths provides the best performance and use that path. For this reason, and for backup in 
  case of a failure, we suggest providing <em>both</em> XCF signaling structures and CTCs for XCF cross-site communication. 
</p>
<h4 id="HMC_and_consoles"><b>HMC and consoles</b></h4>
<p>
  To be able to control the processors in the remote center, you need to have access to the LAN containing 
  the SEs and HMCs for the processors in that location. Such connectivity is typically achieved using bridges or routers. 
</p>
<p>
  If you are running systems at the remote site, you will also want to be able to have consoles for those systems. 
  Two options are 2074 control units and OSA-ICC cards. Alternatively, you can use SNA consoles, but be aware that 
  they cannot be used until IBM VTAM® is started, so they cannot be used for initial system loading.
</p>
<h4 id="Connectivity_options"><b>Connectivity options</b></h4>
<!-- A formatted note -->

<div class="note-container">
  <p role="note" class="info_repeated_multiple_chapters">
    <strong>Note: </strong>WAN connectivity options are not covered in this book. 
    <a href="#Table_2_-_2">Table 2-2</a>, with the exception of HMC connectivity, is predominantly related to disk replication solutions.
  </p>
</div>


<p>
  Now that we have explained what you need to connect across the two sites, we briefly review the most common options 
  for providing that connectivity. There are several ways to provide all this connectivity, from direct channel connection 
  through to DWDMs. 
  <a href="#Table_2_-_2">Table 2-2</a> lists the different options. The distance supported 
  varies by device type and connectivity method.
</p>


<!-- describing the table -->
<p id="Table_2_-_2">
  <i>Table 2-2 Supported CF link distances</i>
</p>

<!-- Table with GRID FOR LAYOUT -->

<!-- GRIDS - one for larger screens and other for smaller - toggle display 'none' in styles.css -->

<!-- GRID - for larger screens  -->
  
<div class="cross-site-connectivity-options-grid-large">
  <div class="grid-item-cross-site-connectivity-options-1"><b>Connection type</b></div>
  <div class="grid-item-cross-site-connectivity-options-2"><b>Direct (un-<br>repeated)</b></div>
  <div class="grid-item-cross-site-connectivity-options-3"><b>Switch and director or cascaded directors</b></div>
  <div class="grid-item-cross-site-connectivity-options-4"><b>DWDM</b></div>
  <div class="grid-item-cross-site-connectivity-options-5"><b>Channel extender</b></div>
  <div class="grid-item-cross-site-connectivity-options-6">Server to disk</div>
  <div class="grid-item-cross-site-connectivity-options-7">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-8">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-9">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-10">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-11">Disk Remote copy</div>
  <div class="grid-item-cross-site-connectivity-options-12">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-13">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-14">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-15">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-16">Coupling links</div>
  <div class="grid-item-cross-site-connectivity-options-17">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-18">No</div>
  <div class="grid-item-cross-site-connectivity-options-19">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-20">No</div>
  <div class="grid-item-cross-site-connectivity-options-21">STP (coupling links)</div>
  <div class="grid-item-cross-site-connectivity-options-22">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-23">No</div>
  <div class="grid-item-cross-site-connectivity-options-24">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-25">No</div>
  <div class="grid-item-cross-site-connectivity-options-26">XCF signaling</div>
  <div class="grid-item-cross-site-connectivity-options-27">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-28">Yes (CTC)
    No (coupling links)</div>
  <div class="grid-item-cross-site-connectivity-options-29">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-30">Yes (CTC only)
    No (coupling links)</div>
  <div class="grid-item-cross-site-connectivity-options-31">HMC/<br>consoles</div>
  <div class="grid-item-cross-site-connectivity-options-32">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-33">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-34">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-35">Yes</div>

</div>

<!-- GRID - for smaller screens  -->

<div class="cross-site-connectivity-options-grid-small">
  <div class="grid-item-cross-site-connectivity-options-1"><b>Connection type</b></div>
  <div class="grid-item-cross-site-connectivity-options-6">Server to disk</div>
  <div class="grid-item-cross-site-connectivity-options-11">Disk Remote copy</div>
  <div class="grid-item-cross-site-connectivity-options-16">Coupling links</div>
  <div class="grid-item-cross-site-connectivity-options-21">STP (coupling links)</div>
  <div class="grid-item-cross-site-connectivity-options-26">XCF signaling</div>
  <div class="grid-item-cross-site-connectivity-options-31">HMC/consoles</div>

  <div class="grid-item-cross-site-connectivity-options-2"><b>Direct (unrepeated)</b></div>
  <div class="grid-item-cross-site-connectivity-options-7">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-12">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-17">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-22">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-27">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-32">Yes</div>

  <div class="grid-item-cross-site-connectivity-options-3"><b>Switch and director or cascaded directors</b></div>
  <div class="grid-item-cross-site-connectivity-options-8">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-13">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-18">No</div>
  <div class="grid-item-cross-site-connectivity-options-23">No</div>
  <div class="grid-item-cross-site-connectivity-options-28">Yes (CTC)
    No (coupling links)</div>
  <div class="grid-item-cross-site-connectivity-options-33">Yes</div>

  <div class="grid-item-cross-site-connectivity-options-4"><b>DWDM</b></div>
  <div class="grid-item-cross-site-connectivity-options-9">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-14">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-19">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-24">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-29">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-34">Yes</div>



  <div class="grid-item-cross-site-connectivity-options-5"><b>Channel extender</b></div>
  <div class="grid-item-cross-site-connectivity-options-10">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-15">Yes</div>
  <div class="grid-item-cross-site-connectivity-options-20">No</div>
  <div class="grid-item-cross-site-connectivity-options-25">No</div>
  <div class="grid-item-cross-site-connectivity-options-30">Yes (CTC only)
    No (coupling links)</div>
  <div class="grid-item-cross-site-connectivity-options-35">Yes</div>

</div>



<!-- END OF GRID -->

<div>



<p>
  For more information about options and distances that are possible, see <i>IBM System 
    z Connectivity Handbook</i>, SG24-5444.
</p>

<h5 id="FICON_switches/directors"><b>FICON switches/directors</b></h5>
<p>
  For more information about IBM Z qualified FICON and Fibre Channel 
  Protocol (FCP) products and products that support mixing FICON and FCP within 
  the same physical FC switch or FICON director, see the 
  <a href="https://www.ibm.com/it-infrastructure">I/O Connectivity web page.</a>
</p>
<p>
  The maximum unrepeated distance for FICON is typically 10 km. However, 
  FICON switches can be used to extend the distance from the server to the control 
  unit further with the use of a cascaded configuration. The maximum supported distance 
  for the interswitch links (ISL) in this configuration is technology- and vendor-specific.  
</p>
<p>
  No matter what the case might be, if the property between the two sites is not owned by 
  your organization, you will need a vendor to provide dark fiber between the two sites because 
  FICON switches/directors cannot be directly connected to telecommunication lines.
</p>
<p>
  For more information, see <i>IBM System z Connectivity Handbook</i>, SG24-5444.
</p>

<h5 id="Wavelength_Division_Multiplexing"><b>Wavelength Division Multiplexing</b></h5>
<p>
  A Wavelength Division Multiplexor (WDM) is a high-speed, high-capacity, scalable fiber optic 
  data transport system that uses Dense Wavelength Division Multiplexing (DWDM) or Course Wavelength 
  Division Multiplexing (CWDM) technology to multiplex several independent bit streams over a single 
  fiber link, thereby making optimal use of the available bandwidth.     
</p>
<p>
  WDM solutions that support the protocols described in this book generally support metropolitan 
  distances in the range of tens to a few hundred kilometers. The infrastructure requirements and 
  the supported distances vary by vendor, model, and even by features on a given model. 
</p>
<p>
  More specifically, several qualified WDM solutions support the following key protocols used in a GDPS solution:
</p>

<ul>
  <li class="arrows">
    Fiber Connection (FICON)
  </li>
  <li class="arrows">
    InterSystem Channel (ISC-3)
  </li>
  <li class="arrows">
    Parallel Sysplex InfiniBand (PSIFB) Long Reach links
  </li>
  <li class="arrows">
    Server Time Protocol (STP) over ISC-3 Peer Mode or PSIFB Long Reach
  </li>
  <li class="arrows">
    Potentially, protocols that are not IBM Z protocols
  </li>
</ul>



<p>
  Given the criticality of these links for transport of data and timing 
  information, it is important to use only qualified WDM vendor solutions 
  when extending Parallel Sysplexes to more than one site (as is often 
  done as part of a GDPS configuration).
</p>

          
<p>
  The latest list of qualified WDM vendor products, along with links to corresponding 
  IBM Redpaper publications for each product, is available at the 
  <a href="https://www-40.ibm.com/servers/resourcelink/">IBM Resource Link web page</a> (sign-in required).     
</p>
<p>
  Also see “Hardware products for servers” on the Library page.   
</p>

<h5 id="Channel_extenders"><b>Channel extenders</b></h5>

<p>
  Channel extenders are special devices that are connected in the path between a server and a control 
  unit, or between two control units. Channel extenders provide the ability to extend connections 
  over much greater distances than that provided by DWDM. Distances supported with channel 
  extenders are virtually unlimited. <em>(For more information about the impact of distance on response times when 
    using channel extenders, contact your IBM representative to obtain the white paper titled, The effect of IU 
    pacing on XRC FICON performance at distance.)</em> 
</p>
<p>
  Unlike DWDMs, channel extenders support connection to telecom lines, removing the need for dark fiber. This can make 
  channel extenders more flexible because access to high-speed telecoms is often easier to obtain than access to dark fiber. 
</p>

          
<p>
  However, channel extenders typically do not support the same range of protocols as DWDMs. In a IBM Z context, 
  channel extenders support IP connections (for example, connections to OSA adapters), FCP and FICON channels, 
  but not coupling links or time synchronization-related links.    
</p>
<p>
  For much more detailed information about the options and distances that are possible, 
  see <em>IBM System z Connectivity Handbook</em>, SG24-5444.
</p>
<p>
  More information about channel extenders that have been qualified to work with IBM storage is available to download from the 
  DS8000 Series Copy Services Fibre Channel Extension Support Matrix 
  <a href="https://www.ibm.com/support/pages/ds8000-series-copy-services-fibre-channel-extension-support-matrix">web page</a>.
</p>

<h4 id="Single_points_of_failure"><b>Single points of failure</b></h4>
  <p>
  When planning to connect systems across sites, it is vital to do as much as you possibly 
  can to avoid all single points of failure. Eliminating all single points of failure makes it significantly easier to 
  distinguish between a connectivity failure and a failure of the remote site. The recovery actions you take are quite 
  different, depending on whether the failure you just detected is a connectivity failure or a real site failure.
  </p>
  <p>
    If you have only a single path, you do not know if it was the path or the remote site that went down. If you 
    have no single points of failure and everything disappears, there is an extremely good chance that it was 
    the site that went down. Any other mechanism to distinguish between a connectivity failure and a site 
    failure (most likely human intervention) cannot react with the speed required to drive effective recovery actions.
  </p>
  <h3 id="Testing_considerations"><b>Testing considerations</b></h3>
  <p>Testing your DR solution is a required and essential step in maintaining DR readiness. Many enterprises have 
    business or regulatory requirements to conduct periodic tests to ensure the business is able to recover from a 
    wide-scale disruption and recovery processes meet RTO and RPO requirements. The only way to determine the effectiveness 
    of the solution and your enterprise's ability to recover from a disaster is through comprehensive testing.
  </p>
  <p>
    One of the most important test considerations in developing a DR test plan is to make sure that the testing you 
    conduct truly represents the way you would recover your data and enterprise. This way, when you actually 
    need to recover following a disaster, you can recover the way you have been testing, thus improving the 
    probability that you will be able to meet the RTO and RPO objectives established by your business.
  </p>
  <h5><b>Testing disk mirroring-based solutions</b></h5>
    <p>When conducting DR drills to test your recovery procedures, without additional disk capacity to 
          support FlashCopy, the mirroring environment will be suspended so the secondary disks can be used to 
          test your recovery and restart processes. When testing is completed, the mirror must be brought back 
          to a duplex state again. During this window, until the mirror is back to a duplex state, the enterprises 
          ability to recover from a disastrous event is compromised.
    </p>
    <p>If this is not acceptable or your enterprise has a requirement to perform periodic disaster recovery tests while maintaining 
      a disaster readiness posture, you will need to provide additional disk capacity to support FlashCopy. The additional FlashCopy 
      device can be used for testing your recovery and restart procedures while the replication environment is running. 
      This ensures that a current and consistent copy of the data is available, and that disaster readiness is maintained 
      throughout the testing process.
    </p>
    <p>
      The additional FlashCopy disk can also be used to create a copy of the secondary devices to ensure a consistent copy of the 
      data is available if a disaster-type event occurs during primary and secondary volume resynchronization.
    </p>
    <p>
      From a business perspective, installing the additional disk capacity to support FlashCopy will mean incurring additional 
      expense. Not having it, however, can result in compromising the enterprise’s ability to recover from a disastrous event, 
      or in extended recovery times and exposure to additional data loss.
    </p>
    <h5><b>Testing software replication solutions</b></h5>
    <p>
      Similar in some instances to the situation described for testing disk-based mirroring solutions, if you test on your target 
      copy of your database or databases, you will have to pause the replication process. Potentially, you might have to also 
      re-create the target copy from scratch by using the source copy as input when the test is complete.
    </p>
    <p>
      It would be normal to test the recovery procedures and operational characteristics of a software replication solution in 
      a pre-production environment that as close as possible reflects the production environment.
    </p>
    <p>
      However, because of the nature of software replication solutions, there is limited recovery required in the target site. 
      Updates will either have been sent (and applied) from the source site, or they will not; the apply process is based on 
      completed units of work, so there should be no issue with incomplete updates arriving from the source site. The testing 
      is more likely to be related to the process for handling the potential data loss and any possible handling of 
      collisions caused by the later capture/apply of stranded transactions with other completed units of work that 
      might have occurred following an outage or disaster.
    </p>
    <h5><b>Testing methodology</b></h5>
    <p>
      How you approach your DR testing is also an important consideration. Most enterprises aim to do the majority of 
      disruptive testing in a test or “sandbox” environment. This ideally will closely resemble the production 
      environment so that the testing scenarios done in the sandbox are representative of what is applicable 
      also in your production environment.
    </p>
    <p>
      Other enterprises might decide to simulate a disaster in the production environment to really prove the 
      processes and technology deliver what is required. Remember, however, that a disaster can surface to the 
      technology in different ways (for example, different components failing in different sequences), so the 
      scenarios you devise and test should consider these possible variations.
    </p>
    <p>
      A typical approach to DR testing in production is to perform some form of a planned site switch. In such a test, 
      the production service is closed down in a controlled manner where it normally runs, and then restarted 
      in the DR site. This type of test will demonstrate that the infrastructure in the DR site is capable of 
      running the services within the scope of the test, but given the brief duration of such tests 
      (often over a weekend only) not all possible workload scenarios can be tested.
    </p>
    <p>
      For this reason, consider the ability to move the production services to the DR site for an extended period 
      (weeks or months), to give an even higher degree of confidence. This ability to “toggle” production and DR 
      locations can provide other operational benefits, such as performing a preemptive switch because of an 
      impending event, along with increased confidence being able to run following a DR invocation.
    </p>
    <p>
      With this approach it is important to continue to test the actual DR process in your test environment, 
      because a real disaster is unlikely to happen in a way where a controlled shutdown is possible. 
      Those processes must then be carefully mapped across to the production environment to ensure success in a DR invocation.
    </p>
    <p>
      In some industries, regulation might dictate or at least suggest guidelines about what constitutes a valid DR test, 
      and this also needs to be considered.
    </p>
    <h4 id="summary_chapter2"><b>Summary</b></h4>
    <p>In this chapter we covered the major building blocks of an IT resilience solution. We discussed providing continuous 
      availability for normal operations, the options for keeping a consistent offsite copy of your disk and tape-based data, 
      the need for automation to manage the recovery process, and the areas you need to consider when connecting across sites.
    </p>
    <p>
      In the next few chapters, we discuss the functions provided by the various offerings in the GDPS family.
    </p>

  </div>

<!-- END OF CHAPTER 2 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
  <div class="prev-next">
    <div class="button-type-container"></div>
        <a href="#Infrastructure_planning_for_availability_and_GDPS" class="button-type chapter-2">Top of section</a>

      <div class="button-type-container"></div>
        <a href="#GDPS_Metro" class="button-type to-chapter-3">Next »</a>
      
      <div class="button-type-container"></div>
        <a href="#GDPS_Metro_HyperSwap_Manager" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

      <div class="button-type-container"></div>
        <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

      <div class="button-type-container"></div>
        <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
    </div>
     
  </section>
</div> 
<hr>




<!-- chapter 3 content -->

<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section"  id="GDPS_Metro">
    <header class="main_chapter_heading">
      <h2><b>GDPS Metro</b></h2>
    </header>
      <code></code>
      <ul>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
        <li></li>
      </ul>
    <div class="header-footer-text-container-padding">
      
      <h5 id="FB_disk_management_prerequisites_for_GDPS_Metro"><b>FB disk management prerequisites</b>
      </h5>
      <p>
        GDPS requires the disk subsystems that contain the FB devices to support the z/OS Fixed Block 
        Architecture (zFBA) feature. GDPS runs on z/OS and therefore communicates to the disk subsystems 
        directly over a channel connection. The z/OS Fixed Block Architecture (zFBA) provides GDPS the ability 
        to send the commands that are necessary to manage Metro Mirror and FlashCopy directly to FB devices 
        over a channel connection. It also enables GDPS to receive notifications for certain error conditions 
        (for example, suspension of an FB device pair). These notifications allow the GDPS controlling system 
        to drive autonomic action such as performing a freeze for a mirroring failure.
      </p>
      <h4 id="Protecting_data_integrity_and_data_availability_with_GDPS_Metro"><b>Protecting data integrity 
        and data availability with GDPS Metro</b>
       
      </h4>
      <p>
        In
        <a href="#Data consistency">Data consistency</a>, we point out that data integrity across primary and secondary 
        volumes of data is essential to perform a database restart and accomplish an RTO of less than hour. 
        This section includes details about how GDPS Metro automation provides both data consistency if there 
        are mirroring problems and data availability if there are primary disk problems.
      </p>
      <h5 id="GDPS_HyperSwap_function_chapter3"><b>GDPS HyperSwap function</b>
        
      </h5>
      <h4 id="Protecting_tape_data"><b>Protecting tape data</b>
       
      </h4>


      <h3><b>FULL CONTENT FOR CHAPTER 3 ISN'T IN YET</b></h3>
    </div>

    <div class="header-footer-text-container-padding">
      <p>
      </p>

          
      <p>
      </p>
      <p>
      </p>
    </div>

<!-- END OF CHAPTER 3 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#GDPS_Metro" class="button-type chapter-3">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#GDPS_Metro_HyperSwap_Manager" class="button-type to-chapter-4">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#GDPS_Global_-_XRC" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>

</div>
  </section>



<!-- chapter 4 content -->

  <div class="chapter-box-shadow chapter-box-margin color-text-bground">
    <section class="main-section" id="GDPS_Metro_HyperSwap_Manager">
      <header class="main_chapter_heading">
        <h2><b>GDPS Metro HyperSwap
          Manager</b></h2>
      </header>
        <code></code>
        <ul>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
        </ul> 
      <div class="header-footer-text-container-padding">
        <h5 id="FB_disk_management_prerequisites_for_GDPS_Metro_HyperSwap_Manager"><b>FB disk management prerequisites</b>
        </h5>
        <p>
          GDPS requires the disk subsystems that contain the FB devices to support the z/OS Fixed Block Architecture 
          (zFBA) feature. GDPS runs on z/OS and therefore communicates to the disk subsystems directly over a channel 
          connection. The z/OS Fixed Block Architecture (zFBA) provides GDPS the ability to send the commands that 
          are necessary to manage Metro Mirror and FlashCopy directly to FB devices over a channel connection.

        </p>
        <p>
          It also enables GDPS to receive notifications for specific error conditions (for example, suspension of an 
          FB device pair). These notifications allow the GDPS controlling system to drive autonomic action, such as 
          performing a freeze for a mirroring failure.
        </p>
        <h4 id="Protecting_data_integrity_and_data_availability_with_GDPS_HM"><b>Protecting data 
          integrity and data availability with GDPS HM</b>
          
        </h4>
        <p>
          In 2.2, “Data consistency” on page 17, we point out that data integrity across primary and 
          secondary volumes of data is essential to perform a database restart and accomplish an RTO of 
          less than an hour. This section provides details about how GDPS automation in GDPS HM provides 
          both data consistency if there are mirroring problems and data availability if there are disk problems.
         
        </p>
        <p>
          The following types of disk problems trigger a GDPS automated reaction:
        </p>
        <h5 id="GDPS_HyperSwap_function_chapter4"><b>GDPS HyperSwap function</b>
        
        </h5>
        <h3><b>FULL CONTENT FOR CHAPTER 4 ISN'T IN YET</b></h3>
      </div>

<!-- END OF CHAPTER 4 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#GDPS_Metro_HyperSwap_Manager" class="button-type chapter-4">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#GDPS_Global_-_XRC" class="button-type to-chapter-5">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#GDPS_Global_-_GM" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>

  </div>
    </section>
 
<!-- chapter 5 content -->

  <div class="chapter-box-shadow chapter-box-margin color-text-bground">
    <section class="main-section" id="GDPS_Global_-_XRC">
      <header class="main_chapter_heading">
        <h2><b>GDPS Global - XRC</b></h2>
      </header>
          <code></code>
          <ul>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
          </ul>
    
    
        <div class="header-footer-text-container-padding">
          <h5></h5>
           
        </div>
      
        <div class="header-footer-text-container-padding">
            
          
        </div>

<!-- END OF CHAPTER 5 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#GDPS_Global_-_XRC" class="button-type chapter-5">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#GDPS_Global_-_GM" class="button-type to-chapter-6">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#GDPS_Continuous_Availability_solution" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>
  </div>
    </section>
 
<!-- chapter 6 content -->

  <div class="chapter-box-shadow chapter-box-margin color-text-bground">
    <section class="main-section" id="GDPS_Global_-_GM">
      <header class="main_chapter_heading">
        <h2><b>GDPS Global - GM</b></h2>
      </header>
          <code></code>
          <ul>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
          </ul>
          
        <div class="header-footer-text-container-padding">            
            <h5></h5>
           
        </div>
      
        <div class="header-footer-text-container-padding">
            

        </div>
<!-- END OF CHAPTER 6 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#GDPS_Global_-_GM" class="button-type chapter-6">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#GDPS_Continuous_Availability_solution" class="button-type to-chapter-7">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#GDPS_Virtual_Appliance" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>


      </section>
  </div>



<!-- chapter 7 content -->

<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section" id="GDPS_Continuous_Availability_solution">
    <header class="main_chapter_heading">
      <h2><b>GDPS Continuous Availability solution</b></h2>
    </header>
        <code></code>
        <ul>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
        </ul>
        
      <div class="header-footer-text-container-padding">            
          <h5></h5>
          
      </div>
    
      <div class="header-footer-text-container-padding">
          

      </div>

<!-- END OF CHAPTER 7 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#GDPS_Continuous_Availability_solution" class="button-type chapter-7">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#GDPS_Virtual_Appliance" class="button-type to-chapter-8">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>


    </section>
</div>




<!-- chapter 8 content -->


<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section" id="GDPS_Virtual_Appliance">
    <header class="main_chapter_heading">
      <h2><b>GDPS Virtual Appliance</b></h2>
    </header>
        <code></code>
        <ul>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
        </ul>
  
  
      <div class="header-footer-text-container-padding">
        <h5 id="GDPS_HyperSwap_function_chapter8"><b>GDPS HyperSwap function</b>
        
        </h5>
        <h3><b>FULL CONTENT FOR CHAPTER 8 ISN'T IN YET</b></h3>
      </div>
    
      <div class="header-footer-text-container-padding">
          
        
      </div>

<!-- END OF CHAPTER 8 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#GDPS_Virtual_Appliance" class="button-type chapter-8">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery" class="button-type to-chapter-9">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#GDPS_Logical_Corruption_Protection_and_Testcopy_Manager" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>
  </section>
</div>
<hr>









<!-- chapter 9 content -->

<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section" id="Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery">
    <header class="main_chapter_heading">
      <h2><b>Combining local and metro continuous availability with out of region disaster recovery</b></h2>
    </header>
        <code></code>
        <ul>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
        </ul>
        
      <div class="header-footer-text-container-padding">            
          <h5></h5>
          
      </div>
    
      <div class="header-footer-text-container-padding">
        <h3><b>FULL CONTENT FOR CHAPTER 9 ISN'T IN YET</b></h3>

      </div>

<!-- END OF CHAPTER 9 the 5 button type links to jump to previous chapter, the next chapter and the top and bottom of the document -->
<div class="prev-next">
  <div class="button-type-container"></div>
    <a href="#Combining_local_and_metro_continuous_availability_with_out_of_region_disaster_recovery" class="button-type chapter-9">Top of section</a>

  <div class="button-type-container"></div>
    <a href="#GDPS_Logical_Corruption_Protection_and_Testcopy_Manager" class="button-type to-chapter-10">Next »</a>
  
  <div class="button-type-container"></div>
    <a href="#Sample_continuous_availability_and_disaster_recovery_scenarios" class="button-type to-skip-to-next-chapter">Skip to Next »</a>

  <div class="button-type-container"></div>
    <a href="#IBM_GDPS_An_Introduction_to_Concepts_and_Capabilities" class="button-type to-top">To top</a>

  <div class="button-type-container"></div>
    <a href="#last_chapter_for_now" class="button-type to-bottom">To bottom</a>
</div>

    </section>
</div>


<!-- chapter 10 content -->

<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section" id="GDPS_Logical_Corruption_Protection_and_Testcopy_Manager">
    <header class="main_chapter_heading">
      <h2><b>GDPS Logical Corruption Protection and Testcopy Manager</b></h2>
    </header>
        <code></code>
        <ul>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
        </ul>
        
      <div class="header-footer-text-container-padding">            
          <h5 id="Introduction_to_LCP_and_Testcopy_Manager"><b>Introduction to LCP and Testcopy Manager</b></h5>
          
      </div>
    
      <div class="header-footer-text-container-padding">
        <h3><b>FULL CONTENT FOR CHAPTER 10 ISN'T IN YET</b></h3>
        

      </div>
    </section>
</div>


<!-- chapter 11 content -->

<div class="chapter-box-shadow chapter-box-margin color-text-bground">
  <section class="main-section" id="Sample_continuous_availability_and_disaster_recovery_scenarios">
    <header class="main_chapter_heading">
      <h2><b>Sample continuous availability and disaster recovery scenarios</b></h2>
    </header>
        <code></code>
        <ul>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
          <li></li>
        </ul>
        
      <div class="header-footer-text-container-padding">            
          <h5></h5>
          
      </div>
    
      <div class="header-footer-text-container-padding">
        <h3 id="last_chapter_for_now"><b>FULL CONTENT FOR CHAPTER 11 ISN'T IN YET</b></h3>

      </div>
    </section>
</div>
      

<!-- END of chapters entries -->

</main>
  
<!-- END GRID -->


<!-- END fixed-size-centered-content -->


<!-- Footer -->
<!-- Labels / tags -->
<!-- Class of footer-background for the color of Footer box -->
<!-- Class of footer-margin-top controlling space over footer box -->
<!-- Class of footer-tags controlling display and spacing of tags in footer box -->
<!-- Classes of black-tag-bground, tag-margin-bottm, and tag-text-smaller -->

<footer class="header-footer-text-container-padding footer-background padding-footer-header footer-margin-top">
  <div>
    <h4>IBM</h4>
  </div>
  <div class="header-footer-text-container-padding color-text-bground">
  <p><span class="footer-tags black-tag-bground tag-margin-bottom">Products & Solutions</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">Learn about</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">Popular links</span>
    <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">About IBM</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">Follow IBM</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span>
    <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span>
    <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span>
    <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span> <span class="footer-tags light-grey tag-text-smaller tag-margin-bottom">?</span>
  </p>
  </div>

<!-- more formatting to be added obviously to the footer etc --> 

  <p>IBM GDPS: An Introduction to Concepts and Capabilities <a href="https://www.ibm.com/docs/en" target="_blank">IBM docs</a>
  </p>
</footer>


<script src="main.js"></script>
</body>
</html>